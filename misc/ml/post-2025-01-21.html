<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-05-05 Mon 13:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AANN 21/01/2024</title>
<meta name="author" content="Alexander E. Zarebski" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="icon" type="image/png" href="../../resources/nicemacs-favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css">
<link rel="stylesheet" href="../../microgram.css">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AANN 21/01/2024</h1>
<div class="nav-buttons">
  <a class="button button-outline nav-button" href="./post-2024-12-28.html">Previous</a>
  <a class="button button-outline nav-button" href="../../index.html">Home</a>
  <a class="button button-outline nav-button" href="./index.html">Index</a>
  <a class="button button-outline nav-button" href="./post-2025-01-21.html">Next</a>
</div>

<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge8783dc">Conditional Real NVP</a>
<ul>
<li><a href="#org0e4e196">Overview</a></li>
<li><a href="#org2351787">Conditional normalizing flows</a></li>
<li><a href="#org7574649">Training</a></li>
<li><a href="#org70a834b">Example</a></li>
<li><a href="#org3a6a1b9">Discussion</a></li>
<li><a href="#orga456560">Thanks</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-orge8783dc" class="outline-2">
<h2 id="orge8783dc">Conditional Real NVP</h2>
<div class="outline-text-2" id="text-orge8783dc">

<div id="org9b8d351" class="figure">
<p><img src="./example-2025-01-21/cover-image.webp" alt="cover-image.webp" width="400px" />
</p>
</div>
</div>
<div id="outline-container-org0e4e196" class="outline-3">
<h3 id="org0e4e196">Overview</h3>
<div class="outline-text-3" id="text-org0e4e196">
<p>
In this post, we implement a <b>conditional</b> version of the Real NVP
(Real-valued Non-Volume Preserving) flows we saw in the <a href="./post-2024-12-28.html">last post</a>. A
conditional normalizing flow is a step towards implementing a full
copy of BayesFlow.
</p>

<p>
Recall that normalizing flows are invertible mappings back and forth
between a target distribution and a base distribution. The base
distribution is something friendly, like a standard multivariate
normal distribution (hence <i>normalizing</i>), and the target is the
unknown distribution you care about (the data-generating process).
</p>

<p>
The idea with the <i>conditional</i> real NVP is that you want to be able
to specify which of a family of distributions is your target using a
context vector. When used for Bayesian statistics, the context vector
summarizes our data, and the target distribution is the resulting
posterior distribution (assuming a fixed prior distribution).
</p>
</div>
</div>
<div id="outline-container-org2351787" class="outline-3">
<h3 id="org2351787">Conditional normalizing flows</h3>
<div class="outline-text-3" id="text-org2351787">
</div>
<div id="outline-container-org00db99a" class="outline-4">
<h4 id="org00db99a">Stock-standard normalizing flows</h4>
<div class="outline-text-4" id="text-org00db99a">
<p>
Consider a random variable \(X\) with an unknown target density
\(p_X(x)\). Typically, \(X\) represents high-dimensional or complex
data, such as the pixel values in an image, making it challenging to
model or manipulate directly. Normalizing flows let us work with \(X\)
indirectly, through a more convenient random variable \(Z\). The
random variable \(Z\) comes from a simpler base distribution
\(p_Z(z)\), e.g., a multivariate Gaussian. The goal is to construct an
invertible function \(f\) that relates \(X\) and \(Z\) through \(X =
f(Z)\). The inverse of \(f\), denoted \(g = f^{-1}\), allows us to map
data from the \(X\)-space back to the \(Z\)-space.
</p>

<p>
Assuming we have the function \(g\), we can calculate the density
\(p_X(x)\) in terms of the \(p_Z(z)\) via the change of variables
formula:
</p>

<p>
\[
p_X(x) = p_Z(g(x)) \left|\det \mathbf{J}(x)\right|,
\]
</p>

<p>
where \(\mathbf{J}(x)\) is the Jacobian of the inverse transformation
\(g\). In the Jacobian, the entry \((i, j)\) represents the partial
derivative \(\frac{\partial g_i(x)}{\partial x_j}\). Having \(f\) and
\(g\) lets us do two important things: sample from \(p_X(x)\), by
sampling from \(p_Z(z)\) and applying the transformation \(f\); and
evaluate \(p_X(x)\) using the formula above.
</p>

<p>
Normalizing flows approximate \(f\) with a neural network. The network
architecture must be designed so that \(f\) is invertible, the
Jacobian is computationally efficient, and the \(g\) is easy to
compute. Real NVP (Real-valued Non-Volume Preserving) networks have
these properties.
</p>
</div>
</div>
<div id="outline-container-org9942d65" class="outline-4">
<h4 id="org9942d65">Conditional normalizing flows</h4>
<div class="outline-text-4" id="text-org9942d65">
<p>
At a high level, the real NVP looks like a deep neural network
composed of layers, each of which applies to only half of the input
dimensions. These layers are the <i>affine coupling blocks</i> (ACBs). To
provide a context vector to the real NVP, i.e. to parameterize the
target distribution of the real NVP, we extend the ACBs to take a
larger vector. This larger vector results from concatenating the
output of the previous ACB with the context vector. This way, each ACB
sees the context vector as part of the input. Including the context
vector in this way gives us the <i>conditional</i> ACB as implemented in
the following snippet.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF; font-weight: bold;">class</span> <span style="color: #6434A3;">CondACB</span>(nn.Module):
    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, block_dim: <span style="color: #006FE0;">int</span>, summary_dim: <span style="color: #006FE0;">int</span>):
        <span style="color: #006FE0;">super</span>(CondACB, <span style="color: #0000FF; font-weight: bold;">self</span>).__init__()
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">block_dim</span> = block_dim
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">summ_dim</span> = summary_dim
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">s_nn</span> = nn.Sequential(
            nn.Linear(<span style="color: #0000FF; font-weight: bold;">self</span>.block_dim + <span style="color: #0000FF; font-weight: bold;">self</span>.summ_dim, 32),
            nn.ELU(),
            nn.Linear(32, 32),
            nn.ELU(),
            nn.Linear(32, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim),
            nn.ELU(),
        )
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">b_nn</span> = nn.Sequential(
            nn.Linear(<span style="color: #0000FF; font-weight: bold;">self</span>.block_dim + <span style="color: #0000FF; font-weight: bold;">self</span>.summ_dim, 32),
            nn.ELU(),
            nn.Linear(32, 32),
            nn.ELU(),
            nn.Linear(32, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim),
            nn.ELU()
        )

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, x: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #BA36A5;">x_a</span> = x[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]
        <span style="color: #BA36A5;">x_b</span> = x[:, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim :]
        <span style="color: #BA36A5;">z_a</span> = x_a
        <span style="color: #BA36A5;">summ_rep</span> = summ.repeat(z_a.size(0), 1)
        <span style="color: #BA36A5;">x_a_prime</span> = torch.cat([z_a, summ_rep], dim=1)
        <span style="color: #BA36A5;">z_b</span> = torch.exp(-<span style="color: #0000FF; font-weight: bold;">self</span>.s_nn(x_a_prime)) * (x_b - <span style="color: #0000FF; font-weight: bold;">self</span>.b_nn(x_a_prime))
        <span style="color: #0000FF; font-weight: bold;">return</span> torch.cat([z_a, z_b], dim=1)

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">ln_det_jacobian</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, x: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #BA36A5;">x_a</span> = x[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]
        <span style="color: #BA36A5;">summ_rep</span> = summ.repeat(x_a.size(0), 1)
        <span style="color: #BA36A5;">x_a_prime</span> = torch.cat([x_a, summ_rep], dim=1)
        <span style="color: #0000FF; font-weight: bold;">return</span> torch.<span style="color: #006FE0;">sum</span>(-<span style="color: #0000FF; font-weight: bold;">self</span>.s_nn(x_a_prime), dim=1)

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">reverse</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, z: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #BA36A5;">z_a</span> = z[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]
        <span style="color: #BA36A5;">z_b</span> = z[:, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim :]
        <span style="color: #BA36A5;">x_a</span> = z_a
        <span style="color: #BA36A5;">summ_rep</span> = summ.repeat(z_a.size(0), 1)
        <span style="color: #BA36A5;">z_a_prime</span> = torch.cat([z_a, summ_rep], dim=1)
        <span style="color: #BA36A5;">x_b</span> = torch.exp(<span style="color: #0000FF; font-weight: bold;">self</span>.s_nn(z_a_prime)) * z_b + <span style="color: #0000FF; font-weight: bold;">self</span>.b_nn(z_a_prime)
        <span style="color: #0000FF; font-weight: bold;">return</span> torch.cat([x_a, x_b], dim=1)
</pre>
</div>

<p>
Linking up a sequence of these is then coordinated by the <code>CondRealNVP</code>
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF; font-weight: bold;">class</span> <span style="color: #6434A3;">CondRealNVP</span>(nn.Module):
    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, block_dim: <span style="color: #006FE0;">int</span>, num_flows: <span style="color: #006FE0;">int</span>, summary_dim: <span style="color: #006FE0;">int</span>):
        <span style="color: #006FE0;">super</span>(CondRealNVP, <span style="color: #0000FF; font-weight: bold;">self</span>).__init__()
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">block_dim</span> = block_dim
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">num_flows</span> = num_flows
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">summary_dim</span> = summary_dim
        <span style="color: #0000FF; font-weight: bold;">self</span>.<span style="color: #BA36A5;">flows</span> = nn.ModuleList([CondACB(block_dim, summary_dim) <span style="color: #0000FF; font-weight: bold;">for</span> _ <span style="color: #0000FF; font-weight: bold;">in</span> <span style="color: #006FE0;">range</span>(num_flows)])

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, x: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #0000FF; font-weight: bold;">for</span> f <span style="color: #0000FF; font-weight: bold;">in</span> <span style="color: #0000FF; font-weight: bold;">self</span>.flows:
            <span style="color: #BA36A5;">x</span> = f(x, summ)
            <span style="color: #BA36A5;">x</span> = torch.cat([x[:, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim :], x[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]], dim=1)
        <span style="color: #0000FF; font-weight: bold;">return</span> x

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">ln_base_pdf</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, z: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #0000FF; font-weight: bold;">return</span> -0.5 * torch.<span style="color: #006FE0;">sum</span>(z**2, dim=1)

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">ln_det_jacobian</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, x: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #BA36A5;">ldjs</span> = torch.zeros(x.size(0))
        <span style="color: #0000FF; font-weight: bold;">for</span> f <span style="color: #0000FF; font-weight: bold;">in</span> <span style="color: #0000FF; font-weight: bold;">self</span>.flows:
            <span style="color: #BA36A5;">ldjs</span> += f.ln_det_jacobian(x, summ)
            <span style="color: #BA36A5;">x</span> = f(x, summ)
            <span style="color: #BA36A5;">x</span> = torch.cat([x[:, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim :], x[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]], dim=1)
        <span style="color: #0000FF; font-weight: bold;">return</span> ldjs

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">reverse</span>(<span style="color: #0000FF; font-weight: bold;">self</span>, z: torch.Tensor, summ: torch.Tensor) -&gt; torch.Tensor:
        <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">import pdb; pdb.set_trace()</span>
        <span style="color: #0000FF; font-weight: bold;">for</span> f <span style="color: #0000FF; font-weight: bold;">in</span> <span style="color: #006FE0;">reversed</span>(<span style="color: #0000FF; font-weight: bold;">self</span>.flows):
            <span style="color: #BA36A5;">z</span> = torch.cat([z[:, <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim :], z[:, : <span style="color: #0000FF; font-weight: bold;">self</span>.block_dim]], dim=1)
            <span style="color: #BA36A5;">z</span> = f.reverse(z, summ)
        <span style="color: #0000FF; font-weight: bold;">return</span> z
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7574649" class="outline-3">
<h3 id="org7574649">Training</h3>
<div class="outline-text-3" id="text-org7574649">
<p>
The training of these networks minimizes the negative log-likelihood
of the training data. The loss function is described by the following
class. Note that we also have to pass in the summary tensor to specify
our target.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF; font-weight: bold;">class</span> <span style="color: #6434A3;">NegLogLikelihoodLoss</span>(nn.Module):

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF; font-weight: bold;">self</span>):
        <span style="color: #006FE0;">super</span>(NegLogLikelihoodLoss, <span style="color: #0000FF; font-weight: bold;">self</span>).__init__()

    <span style="color: #0000FF; font-weight: bold;">def</span> <span style="color: #006699;">forward</span>(
        <span style="color: #0000FF; font-weight: bold;">self</span>, x: torch.Tensor, summ: torch.Tensor, flow: CondRealNVP
    ) -&gt; torch.Tensor:
        <span style="color: #BA36A5;">z</span> = flow(x, summ)
        <span style="color: #BA36A5;">ln_base_pdf</span> = flow.ln_base_pdf(z)
        <span style="color: #BA36A5;">ln_det_jacobian</span> = flow.ln_det_jacobian(x, summ)
        <span style="color: #0000FF; font-weight: bold;">return</span> -torch.mean(ln_base_pdf + ln_det_jacobian)
</pre>
</div>

<p>
During the training loop, for each batch of data, we need to pass in
the corresponding summary vectors in addition to the data so the
network learns to generalize across summary vectors.
</p>
</div>
</div>
<div id="outline-container-org70a834b" class="outline-3">
<h3 id="org70a834b">Example</h3>
<div class="outline-text-3" id="text-org70a834b">
<p>
In the example, the training data consists of samples from one of four
normal distributions. These bivariate normal distributions are centred
at each of the points \((1,1),(1,-1),(-1,1)\) and \((-1,-1)\), and
have a standard deviation of \(0.25\) in each dimension (with
independent components). We use the location of the mean as the
summary vector. Figure <a href="#orgeba1aa0">1</a> shows the training data.
</p>


<div id="orgeba1aa0" class="figure">
<p><img src="./example-2025-01-21/training-data.png" alt="training-data.png" width="700px" />
</p>
</div>

<p>
I trained a <i>conditional</i> real NVP network with 4 layers using Adam
with default settings, a learning rate of \(1\times 10^{-3}\) and
mini-batches of size 10 for 20 epochs. The training data consisted of
1,000 samples from each of the 4 bivariate normal distributions. This
isn't as much training as in the previous example, but the data set is
a bit larger and we are not targeting a multimodal distribution so it
should be easier.
</p>

<p>
The output of the trained model is shown in Figure <a href="#org3f3d313">2</a> below. Here we
have sampled 1000 samples from a standard bivariate normal and then
transformed them with the trained model using each of the four summary
vectors seen in training.
</p>


<div id="org3f3d313" class="figure">
<p><img src="./example-2025-01-21/testing-plot.png" alt="testing-plot.png" width="700px" />
</p>
</div>

<p>
We can also wonder how well this will generalize to new summary
vectors. To test this, we gave \((0,2)\) as a summary vector to the
network. The distribution that it then targeted is shown in Figure <a href="#orgf08e0d5">3</a>.
It doesn't do an amazing job, but it does place it in roughly the
correct position.
</p>


<div id="orgf08e0d5" class="figure">
<p><img src="./example-2025-01-21/extrapolation-plot.png" alt="extrapolation-plot.png" width="700px" />
</p>
</div>
</div>
</div>
<div id="outline-container-org3a6a1b9" class="outline-3">
<h3 id="org3a6a1b9">Discussion</h3>
<div class="outline-text-3" id="text-org3a6a1b9">
<p>
In this post, we have looked at implementing a conditional normalizing
flow model. This is still a long way off from a full BayesFlow
implementation, but we are gradually getting there and it demonstrates
the core idea of parameterizing a normalizing flow using a summary
vector (a.k.a. context vector). It was only given four different
summary vectors to learn from so that it got something plausible out
for \((0,2)\) seems impressive to me.
</p>

<p>
I suppose the next logical step is to train the model to generate the
summary vector from sample data too. This isn't conceptually hard, but
it does require connecting up some additional components. It might
also be interesting to look into using a package for normalizing flows
rather than implementing my own models. That might simplify things
substantially&#x2026;
</p>
</div>
</div>
<div id="outline-container-orga456560" class="outline-3">
<h3 id="orga456560">Thanks</h3>
<div class="outline-text-3" id="text-orga456560">
<p>
Thanks to <a href="https://www.mrc-bsu.cam.ac.uk/staff/shaun-seaman">Shaun Seaman</a>, whose questioning prompted me to test how well
these CNFs will generalize to novel summary vectors.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alexander E. Zarebski</p>
<p class="date">Created: 2025-05-05 Mon 13:23</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
