<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-01-12 Fri 18:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Alexander E. Zarebski" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="icon" type="image/png" href="../../resources/nicemacs-favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css">
<link rel="stylesheet" href="../../microgram.css">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<p>
<a href="../../index.html">Home</a>
</p>

<p>
<a href="./index.html">Index</a>
</p>

<div id="outline-container-org389a00b" class="outline-2">
<h2 id="org389a00b">Neural Bayes estimators</h2>
<div class="outline-text-2" id="text-org389a00b">
<p>
In this example we will use the <i>neural Bayes estimators</i> developed by
<a href="https://doi.org/10.1080/00031305.2023.2249522">Sainsbury-Dale et al (2023)</a> for the very simple task of generating a
point estimate of the parameters of a linear regression. Training the
model was a bit harder than in previous MNIST examples, but with a bit
of trial-and-error I was able to get something reasonable that trained
in under 5 minutes. The overall architecture used in the paper is
shown in Figure <a href="#org7efb2c7">1</a>.
</p>


<div id="org7efb2c7" class="figure">
<p><img src="./diagrams/drawing-2024-01-11.png" alt="drawing-2024-01-11.png" width="400px" />
</p>
</div>
</div>

<div id="outline-container-orgb02ae0d" class="outline-3">
<h3 id="orgb02ae0d">A statistical model</h3>
<div class="outline-text-3" id="text-orgb02ae0d">
<p>
The statistical model we are interested in estimating the parameters
of is linear regression:
</p>

<p>
\[
y_i\sim\text{Normal}(\beta x_i, \sigma^{2})
\]
</p>

<p>
for \(i=1,\ldots,m\) where we assume \(\sigma^{2} = 0.5\) is fixed and
known to keep things simple. There are only two covariates so \(x_i =
(1, x_i^1, x_i^2)\) (we assume an unknown intercept). The prior
distribution on the elements of \(\beta\) is a standard normal
distribution.
</p>

<p>
I want to stick to the work in the paper, so I won't attempt any
uncertainty quantification in this example. They suggest that you
could use the bootstrap since parameter estimation is so cheap.
</p>
</div>
</div>

<div id="outline-container-orgb70b232" class="outline-3">
<h3 id="orgb70b232">Loading packages</h3>
<div class="outline-text-3" id="text-orgb70b232">
<p>
Load packages and set the seed used by PyTorch. Since there is
extensive functionality for random number generation in PyTorch, we
will use that to simulate our datasets.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">import</span> time
<span style="color: #0000FF;">import</span> torch
<span style="color: #0000FF;">import</span> torch.nn <span style="color: #0000FF;">as</span> nn
<span style="color: #0000FF;">import</span> torch.optim <span style="color: #0000FF;">as</span> optim
<span style="color: #0000FF;">from</span> torch.distributions <span style="color: #0000FF;">import</span> uniform, normal

<span style="color: #0000FF;">import</span> pandas <span style="color: #0000FF;">as</span> pd
<span style="color: #0000FF;">import</span> plotnine <span style="color: #0000FF;">as</span> p9
<span style="color: #0000FF;">from</span> plotnine <span style="color: #0000FF;">import</span> *

torch.manual_seed(0)
</pre>
</div>
</div>
</div>

<div id="outline-container-org7814b1c" class="outline-3">
<h3 id="org7814b1c">Define the input and output filenames</h3>
<div class="outline-text-3" id="text-org7814b1c">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">training_data_csv</span> = <span style="color: #008000;">"data/linear-regression-2024-01-11-training.csv"</span>
<span style="color: #BA36A5;">testing_data_csv</span> = <span style="color: #008000;">"data/linear-regression-2024-01-11-testing.csv"</span>
<span style="color: #BA36A5;">loss_csv</span> = <span style="color: #008000;">"example-2024-01-11-loss.csv"</span>
<span style="color: #BA36A5;">loss_png</span> = <span style="color: #008000;">"example-2024-01-11-loss.png"</span>
<span style="color: #BA36A5;">beta_png</span> = <span style="color: #0000FF;">lambda</span> n: f<span style="color: #008000;">"example-2024-01-11-beta</span>{n}<span style="color: #008000;">.png"</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org0f2cced" class="outline-3">
<h3 id="org0f2cced">Define statistical model constants</h3>
<div class="outline-text-3" id="text-org0f2cced">
<p>
Since each datum for this model is itself a dataset, we need to define
the size of those datasets. This corresponds to the parameter \(J\) in
the paper. In the paper they suggest a way to allow \(m\) to vary but do
not appear to test it, here it is fixed to <code>dataset_size</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">y_var</span> = torch.tensor([0.5])
<span style="color: #BA36A5;">dataset_size</span> = 500
<span style="color: #BA36A5;">std_normal</span> = normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))
<span style="color: #BA36A5;">uniform_x_range</span> = uniform.Uniform(
    torch.tensor([-5.0]), torch.tensor([5.0])
)
</pre>
</div>
</div>
</div>

<div id="outline-container-org80ffd55" class="outline-3">
<h3 id="org80ffd55">Define simulation constants</h3>
<div class="outline-text-3" id="text-org80ffd55">
<p>
The value of <code>train_reps</code> is the number of datasets that we will
generate, this corresponds loosely to the parameter \(K\) in the paper.
The <code>test_reps</code> is there to define how big the test set is.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">train_reps</span> = 1000
<span style="color: #BA36A5;">train_batch_size</span> = 100
<span style="color: #BA36A5;">train_num_epochs</span> = 10000

<span style="color: #BA36A5;">test_reps</span> = 100
</pre>
</div>
</div>
</div>

<div id="outline-container-orgd4912d4" class="outline-3">
<h3 id="orgd4912d4">Simulate training data</h3>
<div class="outline-text-3" id="text-orgd4912d4">
<p>
Simulating the training data is a little bit fiddly, but the only
notable thing is the use of the <a href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch-bmm">bmm</a> function to do the batch
matrix-matrix product. Since we need to generate both a training and a
testing dataset, it makes sense to write a function to do this.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">def</span> <span style="color: #006699;">random_regression_datasets</span>(
        num_replicates: <span style="color: #006FE0;">int</span>,
        replicate_size: <span style="color: #006FE0;">int</span>,
        model_dimension: <span style="color: #006FE0;">int</span>):
    <span style="color: #BA36A5;">beta_vals</span> = std_normal.sample(
        sample_shape=torch.Size([num_replicates, model_dimension])
    )
    <span style="color: #BA36A5;">x_vals</span> = (
        uniform_x_range
        .sample(
            sample_shape=torch.Size(
                [num_replicates, replicate_size, model_dimension-1]
            ))
        .squeeze(3)
    )
    <span style="color: #BA36A5;">x_vals_full</span> = torch.cat(
        (torch.ones(num_replicates, replicate_size, 1), x_vals),
        dim = 2)
    <span style="color: #BA36A5;">y_mean</span> = torch.bmm(x_vals_full, beta_vals)
    <span style="color: #BA36A5;">y_vals</span> = (
        normal
        .Normal(y_mean, y_var)
        .sample(sample_shape=torch.Size([1]))
        .squeeze(0)
    )
    <span style="color: #BA36A5;">data_vals</span> = torch.cat((y_vals, x_vals), dim=2)
    <span style="color: #0000FF;">return</span> data_vals, beta_vals
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">train_data</span>, <span style="color: #BA36A5;">train_beta</span> = random_regression_datasets(
    train_reps, dataset_size, 3)
<span style="color: #0000FF;">assert</span> train_data.shape == torch.Size([train_reps, dataset_size, 3])
</pre>
</div>
</div>

<div id="outline-container-orgc5b82cd" class="outline-4">
<h4 id="orgc5b82cd">Save the training data to CSV</h4>
<div class="outline-text-4" id="text-orgc5b82cd">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">train_data_df</span> = []
<span style="color: #0000FF;">for</span> ix <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(train_data.shape[0]):
    <span style="color: #0000FF;">for</span> jx <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(train_data.shape[1]):
        train_data_df.append(
            [ix, jx] + train_data[ix][jx].tolist() + train_beta[ix].squeeze(1).tolist()
        )

<span style="color: #BA36A5;">train_data_df</span> = pd.DataFrame(
    train_data_df,
    columns=[<span style="color: #008000;">'replicate'</span>, <span style="color: #008000;">'element'</span>,
             <span style="color: #008000;">'y'</span>, <span style="color: #008000;">'x1'</span>, <span style="color: #008000;">'x2'</span>,
             <span style="color: #008000;">'beta0'</span>, <span style="color: #008000;">'beta1'</span>, <span style="color: #008000;">'beta2'</span>]
)

train_data_df.to_csv(training_data_csv)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd06f983" class="outline-3">
<h3 id="orgd06f983">Define the model architecture</h3>
<div class="outline-text-3" id="text-orgd06f983">
<p>
There are three components to this module: the \(\psi\) network which
summarises a single datum, the \(a\) aggregator which combines these
summaries, and the \(\phi\) network which generates the final
estimates of the parameters.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">LinearRegressionNB</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>, m):
        <span style="color: #006FE0;">super</span>(LinearRegressionNB, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_m</span> = m     <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">individual dataset size: {z_1,...,z_m}</span>
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_n</span> = 3     <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">input dimension: (y_i, x_i^1, x_i^2)</span>
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_q</span> = 10    <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">latent dimension</span>
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_p</span> = 3     <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">output dimension: (beta_0, beta_1, beta_2)</span>

        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_psi</span> = nn.Sequential(
            nn.Linear(<span style="color: #0000FF;">self</span>._n, <span style="color: #0000FF;">self</span>._q),
            nn.Sigmoid(),
        )
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_aggregator</span> = torch.mean
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_phi</span> = nn.Sequential(
            nn.Linear(<span style="color: #0000FF;">self</span>._q, <span style="color: #0000FF;">self</span>._q),
            nn.Sigmoid(),
            nn.Linear(<span style="color: #0000FF;">self</span>._q, <span style="color: #0000FF;">self</span>._p)
        )

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">assert x.dim() == 3</span>
        <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">assert x.shape[1] == self._m</span>
        <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">assert x.shape[2] == self._n</span>

        <span style="color: #BA36A5;">outputs</span> = []
        <span style="color: #0000FF;">for</span> ix <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(x.shape[0]):
            <span style="color: #BA36A5;">x_ix</span> = <span style="color: #0000FF;">self</span>._psi(x[ix])
            <span style="color: #BA36A5;">x_ix</span> = <span style="color: #0000FF;">self</span>._aggregator(x_ix, dim=0)
            <span style="color: #BA36A5;">x_ix</span> = <span style="color: #0000FF;">self</span>._phi(x_ix)
            outputs.append(x_ix)

        <span style="color: #BA36A5;">x</span> = torch.stack(outputs, dim=0)
        <span style="color: #BA36A5;">x</span> = x.unsqueeze(2)
        <span style="color: #0000FF;">return</span> x
</pre>
</div>

<p>
The for-loop is used in the forward method because that is the
cleanest way I could find to broadcast this model over a batch of
datasets. Since the batch size will probably never get to big this
doesn't seem like a terrible choice, but would be worth keeping in
mind.
</p>

<p>
There are some assertions in the forward method that are commented out
so that the code runs as fast as possible during training. They are
useful as a way to check everything is set up correctly so I will
leave them there.
</p>
</div>
</div>

<div id="outline-container-org2a26998" class="outline-3">
<h3 id="org2a26998">Instantiate the network and check it works as expected</h3>
<div class="outline-text-3" id="text-org2a26998">
<p>
As mentioned above, this network will only work for datasets of a
fixed size, so we need to specify this at run time when the network is
initialised.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">lm_nb</span> = LinearRegressionNB(dataset_size)
<span style="color: #0000FF;">assert</span> lm_nb(train_data).shape == torch.Size([train_reps, 3, 1])
<span style="color: #0000FF;">assert</span> lm_nb(train_data).shape == train_beta.shape
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb1d58ab" class="outline-3">
<h3 id="orgb1d58ab">Train the network</h3>
<div class="outline-text-3" id="text-orgb1d58ab">
<p>
Since this is a regression problem we will use the MSE loss function.
We manage the batching of the data manually. Since it is simulated, it
would be possible to generate it on the fly from a data loader.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">loss_fn</span> = nn.MSELoss()
<span style="color: #BA36A5;">optimizer</span> = optim.Adam(lm_nb.parameters(), lr=1e-3)

lm_nb.train()
<span style="color: #BA36A5;">loss_history</span> = []
<span style="color: #BA36A5;">training_start_time</span> = time.time()
<span style="color: #0000FF;">for</span> epoch <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(train_num_epochs):
    <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Generate a random sample of 100 indicies to use as a batch</span>
    <span style="color: #BA36A5;">batch_idx</span> = torch.randint(0, train_reps, (train_batch_size,))
    <span style="color: #BA36A5;">batch</span> = train_data[batch_idx]
    <span style="color: #BA36A5;">batch_beta</span> = train_beta[batch_idx]

    <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Forward pass</span>
    <span style="color: #BA36A5;">batch_ests</span> = lm_nb(batch)
    <span style="color: #BA36A5;">loss</span> = loss_fn(batch_ests, batch_beta)
    <span style="color: #BA36A5;">epoch_loss</span> = loss.item()

    <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Backward pass</span>
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    <span style="color: #0000FF;">if</span> epoch % 100 == 0:
        <span style="color: #006FE0;">print</span>(f<span style="color: #008000;">"Epoch </span>{epoch}<span style="color: #008000;"> loss: </span>{epoch_loss}<span style="color: #008000;">"</span>)
    loss_history.append((epoch,epoch_loss))
lm_nb.<span style="color: #006FE0;">eval</span>()
<span style="color: #BA36A5;">training_finish_time</span> = time.time()
<span style="color: #006FE0;">print</span>(f<span style="color: #008000;">"Training took: </span>{training_finish_time - training_start_time}<span style="color: #008000;">"</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org7ddd6af" class="outline-3">
<h3 id="org7ddd6af">Visualise the training process</h3>
<div class="outline-text-3" id="text-org7ddd6af">

<div id="orgce8d252" class="figure">
<p><img src="./example-2024-01-11-loss.png" alt="example-2024-01-11-loss.png" width="400px" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">loss_df</span> = pd.DataFrame(loss_history, columns=[<span style="color: #008000;">'epoch'</span>, <span style="color: #008000;">'loss'</span>])
loss_df.to_csv(loss_csv)

(ggplot(loss_df, aes(x=<span style="color: #008000;">'epoch'</span>, y=<span style="color: #008000;">'loss'</span>)) +
    geom_line() +
    theme_bw()
).save(loss_png, height = 2.9, width = 4.1)
</pre>
</div>
</div>
</div>

<div id="outline-container-org676ce10" class="outline-3">
<h3 id="org676ce10">Simulate testing data</h3>
<div class="outline-text-3" id="text-org676ce10">
<p>
We can reuse the function from above to simulate a dataset to test the
model with.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">test_data</span>, <span style="color: #BA36A5;">test_beta</span> = random_regression_datasets(
    test_reps, dataset_size, 3)
<span style="color: #0000FF;">assert</span> test_data.shape == torch.Size([test_reps, dataset_size, 3])
</pre>
</div>
</div>

<div id="outline-container-orga315dc8" class="outline-4">
<h4 id="orga315dc8">Save the testing data to CSV</h4>
<div class="outline-text-4" id="text-orga315dc8">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">test_data_df</span> = []
<span style="color: #0000FF;">for</span> ix <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(test_data.shape[0]):
    <span style="color: #0000FF;">for</span> jx <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(test_data.shape[1]):
        test_data_df.append(
            [ix, jx] + test_data[ix][jx].tolist() + test_beta[ix].squeeze(1).tolist()
        )

<span style="color: #BA36A5;">test_data_df</span> = pd.DataFrame(
    test_data_df,
    columns=[<span style="color: #008000;">'replicate'</span>, <span style="color: #008000;">'element'</span>,
             <span style="color: #008000;">'y'</span>, <span style="color: #008000;">'x1'</span>, <span style="color: #008000;">'x2'</span>,
             <span style="color: #008000;">'beta0'</span>, <span style="color: #008000;">'beta1'</span>, <span style="color: #008000;">'beta2'</span>]
)

test_data_df.to_csv(testing_data_csv)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd007b6e" class="outline-3">
<h3 id="orgd007b6e">Visualise the test results</h3>
<div class="outline-text-3" id="text-orgd007b6e">
<p>
The agreement between the true parameters and the estimates is good,
but with the value of \(m\) that should be expected. It would be nice to
compare this to the MLEs generated by a statistics package in the
future.
</p>


<div id="org7942990" class="figure">
<p><img src="./example-2024-01-11-beta0.png" alt="example-2024-01-11-beta0.png" width="400px" />
</p>
</div>


<div id="org77d7f19" class="figure">
<p><img src="./example-2024-01-11-beta1.png" alt="example-2024-01-11-beta1.png" width="400px" />
</p>
</div>


<div id="orge72d3ae" class="figure">
<p><img src="./example-2024-01-11-beta2.png" alt="example-2024-01-11-beta2.png" width="400px" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">test_beta_df</span> = pd.DataFrame(
    test_beta.squeeze(2).numpy(),
    columns=[<span style="color: #008000;">'true_beta_0'</span>, <span style="color: #008000;">'true_beta_1'</span>, <span style="color: #008000;">'true_beta_2'</span>])
<span style="color: #BA36A5;">test_beta_df</span>[<span style="color: #008000;">'dataset'</span>] = test_beta_df.index
<span style="color: #BA36A5;">test_beta_est_df</span> = pd.DataFrame(
    lm_nb(test_data).squeeze(2).detach().numpy(),
    columns=[<span style="color: #008000;">'est_beta_0'</span>, <span style="color: #008000;">'est_beta_1'</span>, <span style="color: #008000;">'est_beta_2'</span>])
<span style="color: #BA36A5;">test_beta_est_df</span>[<span style="color: #008000;">'dataset'</span>] = test_beta_est_df.index

<span style="color: #BA36A5;">plot_df</span> = pd.merge(test_beta_df, test_beta_est_df, on=<span style="color: #008000;">'dataset'</span>)

(ggplot(plot_df, aes(x=<span style="color: #008000;">'true_beta_0'</span>, y=<span style="color: #008000;">'est_beta_0'</span>)) +
 geom_point() +
    geom_abline(intercept=0, slope=1) +
    theme_bw()
).save(beta_png(0), height = 2.9, width = 4.1)

(ggplot(plot_df, aes(x=<span style="color: #008000;">'true_beta_1'</span>, y=<span style="color: #008000;">'est_beta_1'</span>)) +
    geom_point() +
        geom_abline(intercept=0, slope=1) +
        theme_bw()
    ).save(beta_png(1), height = 2.9, width = 4.1)

(ggplot(plot_df, aes(x=<span style="color: #008000;">'true_beta_2'</span>, y=<span style="color: #008000;">'est_beta_2'</span>)) +
    geom_point() +
        geom_abline(intercept=0, slope=1) +
        theme_bw()
    ).save(beta_png(2), height = 2.9, width = 4.1)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgceb72de" class="outline-3">
<h3 id="orgceb72de">Discussion</h3>
<div class="outline-text-3" id="text-orgceb72de">
<p>
Neural Bayes estimators provide a way to approximate Bayes estimators
with neural networks, yieldling flexible approach to likelihood-free
inference. However, it is not immediately clear what the best way to
quantify the uncertainty is. To an extent, this can be achieved with
<i>simultaneous quantile regression</i> as proposed by <a href="https://doi.org/10.48550/arXiv.1811.00908">Tagasovska and
Lopez-Paz (2019)</a>.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alexander E. Zarebski</p>
<p class="date">Created: 2024-01-12 Fri 18:23</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>