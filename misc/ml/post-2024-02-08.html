<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-21 Fri 17:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AANN 08/02/2024</title>
<meta name="author" content="Alexander E. Zarebski" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="icon" type="image/png" href="../../resources/nicemacs-favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css">
<link rel="stylesheet" href="../../microgram.css">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AANN 08/02/2024</h1>
<div class="nav-buttons">
  <a class="button button-outline nav-button" href="./post-2024-01-25.html">Previous</a>
  <a class="button button-outline nav-button" href="../../index.html">Home</a>
  <a class="button button-outline nav-button" href="./index.html">Index</a>
  <a class="button button-outline nav-button" href="./post-2024-02-22.html">Next</a>
</div>

<div id="outline-container-org31cc063" class="outline-2">
<h2 id="org31cc063">Gated multimodal units</h2>
<div class="outline-text-2" id="text-org31cc063">
<p>
In this example we will use the <i>gated multimodal units</i> (GMU) as
proposed by <a href="https://arxiv.org/abs/1702.01992">Arevalo et al (2017)</a> to solve a classification problem on
a synthetic dataset. The GMU provides a way to combine data from
multiple sources (provided they can be mapped into a shared space).
Since this is about the new type of unit (defined <a href="#gmu">here</a>), we will use
the basic simulation example (defined <a href="#sim">here</a>) from the paper rather than
trying to replicate the full movie genre classification task.
</p>
</div>

<div id="outline-container-sim" class="outline-3">
<h3 id="sim">Synthetic data</h3>
<div class="outline-text-3" id="text-sim">
<p>
The task we consider in the simulation study is simple binary
classification, where we have a pair of univariate predictors in each
sample, and we want to predict a binary class value. In each
observation, only one of the predictors is informative of the class
and the other is pure noise drawn from another distribution<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>.
</p>

<ul class="org-ul">
<li>\(C\sim\text{Bernoulli}(p_C)\) for the class</li>
<li>\(M\sim\text{Bernoulli}(p_M)\) for which modality is informative</li>
<li>Then for modality \(i\) where \(i=a\) or \(b\) we have
<ul class="org-ul">
<li>\(y_i\sim\text{Normal}(\gamma_i^C)\) the informative predictor</li>
<li>\(\tilde{y}_i\sim\text{Normal}(\tilde{\gamma}_i)\) noise</li>
<li>\(x_a = M y_a + (1 - M) \tilde{y}_a\) and \(x_b = (1 - M) y_b + M \tilde{y}_b\).</li>
</ul></li>
</ul>

<p>
So, we have a binary class label \(C\) and we want to predict if it is
\(0\) or \(1\) based on a pair of noisy observations \(x_a\) and
\(x_b\). The random variable \(M\) determines which of \(x_a\) and
\(x_b\) is informative of \(C\) (the other is pure noise).
</p>

<p>
The GMU can handle the more general case where both variables are
informative, but this is the example from the paper so we will stick
with that to keep things simple.
</p>
</div>
</div>

<div id="outline-container-org1e0f753" class="outline-3">
<h3 id="org1e0f753">Loading packages</h3>
<div class="outline-text-3" id="text-org1e0f753">
<p>
First we need to load packages and set the seed for our random number
generator. The <code>current_directory</code> is used to make it easier to load
some simple helper code and can be ignored.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">import</span> time
<span style="color: #0000FF;">import</span> sys
<span style="color: #0000FF;">import</span> os
<span style="color: #0000FF;">from</span> pathlib <span style="color: #0000FF;">import</span> Path
<span style="color: #0000FF;">from</span> typing <span style="color: #0000FF;">import</span> Tuple
<span style="color: #0000FF;">import</span> inspect

<span style="color: #0000FF;">import</span> torch
<span style="color: #0000FF;">import</span> torch.nn <span style="color: #0000FF;">as</span> nn
<span style="color: #0000FF;">import</span> torch.optim <span style="color: #0000FF;">as</span> optim
<span style="color: #0000FF;">from</span> torch.distributions <span style="color: #0000FF;">import</span> normal, bernoulli

<span style="color: #0000FF;">import</span> pandas <span style="color: #0000FF;">as</span> pd
<span style="color: #0000FF;">import</span> plotnine <span style="color: #0000FF;">as</span> p9

<span style="color: #BA36A5;">current_directory</span> = Path(os.getcwd())
sys.path.append(<span style="color: #006FE0;">str</span>(current_directory.parent))
<span style="color: #0000FF;">import</span> niceneuron.plot <span style="color: #0000FF;">as</span> nn_plot

torch.manual_seed(0)
</pre>
</div>

<p>
We import the <code>inspect</code> module here because it provides a clean way to
check the signature of methods. This is helpful later on for writing a
generic training loop below. You can pretty safely ignore this as well
though.
</p>
</div>
</div>

<div id="outline-container-org026ac61" class="outline-3">
<h3 id="org026ac61">Define data generating process</h3>
<div class="outline-text-3" id="text-org026ac61">
<p>
We can implement the model above with some values for the \(p_C\),
\(p_M\) and \(\gamma\) as
</p>

<ul class="org-ul">
<li>\(p_C=0.5\)</li>
<li>\(p_M=0.5\)</li>
<li>\(\gamma^{0}_a=1.0\), \(\gamma^{1}_a=2.0\), \(\tilde{\gamma}_a=3\)</li>
<li>\(\gamma^{0}_b=2.0\), \(\gamma^{1}_b=3.0\), \(\tilde{\gamma}_b=1\)</li>
</ul>

<p>
The following snippet implements the data generating process described
<a href="#sim">above</a>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">def</span> <span style="color: #006699;">rand_dataset</span>(
        num_samples: <span style="color: #006FE0;">int</span>
) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    <span style="color: #BA36A5;">class_labels</span> = (bernoulli
                    .Bernoulli(probs=0.5)
                    .sample((num_samples,)))
    <span style="color: #BA36A5;">modal_labels</span> = (bernoulli
                    .Bernoulli(probs=0.5)
                    .sample((num_samples,)))

    <span style="color: #BA36A5;">l_0</span> = (normal
           .Normal(loc=torch.where(class_labels == 0,
                                   torch.Tensor([1]),
                                   torch.Tensor([2])),
                   scale=torch.Tensor([1]))
           .sample(sample_shape=torch.Size([1])))
    <span style="color: #BA36A5;">l_0_noise</span> = normal.Normal(loc=3, scale=1).sample((num_samples,))
    <span style="color: #BA36A5;">x_0</span> = torch.where(modal_labels == 0, l_0, l_0_noise).squeeze(0)

    <span style="color: #BA36A5;">l_1</span> = (normal
           .Normal(loc=torch.where(class_labels == 1,
                                   torch.Tensor([3]),
                                   torch.Tensor([2])),
                   scale=1)
           .sample(sample_shape=torch.Size([1])))
    <span style="color: #BA36A5;">l_1_noise</span> = normal.Normal(loc=1, scale=1).sample((num_samples,))
    <span style="color: #BA36A5;">x_1</span> = torch.where(modal_labels == 1, l_1, l_1_noise).squeeze(0)
    <span style="color: #0000FF;">return</span> (class_labels.to(torch.<span style="color: #006FE0;">long</span>),
            x_0.unsqueeze(1),
            x_1.unsqueeze(1))
</pre>
</div>
</div>
</div>

<div id="outline-container-org9163d92" class="outline-3">
<h3 id="org9163d92">Define the input and output filenames</h3>
<div class="outline-text-3" id="text-org9163d92">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">num_samples</span> = 5000
<span style="color: #BA36A5;">num_epochs</span> = 7000
<span style="color: #BA36A5;">loss_png</span> = <span style="color: #008000;">"loss.png"</span>
<span style="color: #BA36A5;">loss_csv</span> = <span style="color: #008000;">"loss.csv"</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-gmu" class="outline-3">
<h3 id="gmu">Define the gated multimodal unit</h3>
<div class="outline-text-3" id="text-gmu">
<p>
Here is GMU in its binary form as described by <a href="https://arxiv.org/abs/1702.01992">Arevalo et al (2017)</a>.
The GMU has hidden values
</p>

<p>
\[
h_i = \tanh(W_i \cdot x_i)
\]
</p>

<p>
for \(i=a\) and \(b\), a weighting vector
</p>

<p>
\[
z = \sigma(W_z \cdot [x_a, x_b]),
\]
</p>

<p>
and outputs
</p>

<p>
\[
h = z * h_a + (1 - z) * h_b.
\]
</p>

<p>
The parameters of this unit are \(\Theta = \{W_a, W_b, W_z\}\).
</p>

<p>
And as a PyTorch Module, this is
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">BinaryGMU</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>, input_dim:<span style="color: #006FE0;">int</span>, output_dim:<span style="color: #006FE0;">int</span>):
        <span style="color: #006FE0;">super</span>(BinaryGMU, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">input_dim</span> = input_dim
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">output_dim</span> = output_dim
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_W_a</span> = nn.Linear(<span style="color: #0000FF;">self</span>.input_dim, <span style="color: #0000FF;">self</span>.output_dim)
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_W_b</span> = nn.Linear(<span style="color: #0000FF;">self</span>.input_dim, <span style="color: #0000FF;">self</span>.output_dim)
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_W_z</span> = nn.Linear(<span style="color: #0000FF;">self</span>.output_dim + <span style="color: #0000FF;">self</span>.output_dim, <span style="color: #0000FF;">self</span>.output_dim)

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x_a, x_b):
        <span style="color: #BA36A5;">h_a</span> = torch.tanh(<span style="color: #0000FF;">self</span>._W_a(x_a))
        <span style="color: #BA36A5;">h_b</span> = torch.tanh(<span style="color: #0000FF;">self</span>._W_b(x_b))
        <span style="color: #BA36A5;">z</span> = torch.sigmoid(<span style="color: #0000FF;">self</span>._W_z(torch.cat((h_a, h_b), dim=1)))
        <span style="color: #0000FF;">return</span> z * h_a + (1 - z) * h_b
</pre>
</div>

<p>
Note that we have assume that \(x_a\) and \(x_b\) have the same
dimensionality, but this is not necessary.
</p>
</div>
</div>

<div id="outline-container-org348d810" class="outline-3">
<h3 id="org348d810">Define the network architectures used in the prediction</h3>
<div class="outline-text-3" id="text-org348d810">
<p>
We will consider a couple of different network architectures to
demonstrate the use of the GMU.
</p>
</div>

<div id="outline-container-orgffdd331" class="outline-4">
<h4 id="orgffdd331">Define a univariate model</h4>
<div class="outline-text-4" id="text-orgffdd331">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">DemoUniModel</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>):
        <span style="color: #006FE0;">super</span>(DemoUniModel, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_uni</span> = nn.Sequential(
            nn.Linear(1, 1),
            nn.Tanh(),
            nn.Linear(1, 2)
        )

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #BA36A5;">logits</span> = <span style="color: #0000FF;">self</span>._uni(x)
        <span style="color: #0000FF;">return</span> logits
</pre>
</div>
</div>
</div>

<div id="outline-container-org26b165e" class="outline-4">
<h4 id="org26b165e">Define a model that just concatenates the modalities</h4>
<div class="outline-text-4" id="text-org26b165e">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">DemoCatModel</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>):
        <span style="color: #006FE0;">super</span>(DemoCatModel, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_cat</span> = nn.Sequential(
            nn.Linear(2, 2),
            nn.Tanh(),
            nn.Linear(2, 2)
        )

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x_a, x_b):
        <span style="color: #BA36A5;">logits</span> = <span style="color: #0000FF;">self</span>._cat(torch.cat((x_a, x_b), dim=1))
        <span style="color: #0000FF;">return</span> logits
</pre>
</div>
</div>
</div>

<div id="outline-container-orgae85c23" class="outline-4">
<h4 id="orgae85c23">Define a model that combines the modalities with a GMU</h4>
<div class="outline-text-4" id="text-orgae85c23">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">DemoMultiModel</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>):
        <span style="color: #006FE0;">super</span>(DemoMultiModel, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_gmu</span> = BinaryGMU(input_dim=1, output_dim=2)
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">_W</span> = nn.Linear(2, 2)

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x_a, x_b):
        <span style="color: #BA36A5;">logits</span> = <span style="color: #0000FF;">self</span>._W(<span style="color: #0000FF;">self</span>._gmu(x_a, x_b))
        <span style="color: #0000FF;">return</span> logits
</pre>
</div>
</div>
</div>

<div id="outline-container-org6a66592" class="outline-4">
<h4 id="org6a66592">Instantiate the models</h4>
<div class="outline-text-4" id="text-org6a66592">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">model_uni</span> = DemoUniModel()
<span style="color: #BA36A5;">model_cat</span> = DemoCatModel()
<span style="color: #BA36A5;">model_multi</span> = DemoMultiModel()
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc0b6e15" class="outline-3">
<h3 id="orgc0b6e15">Prepare for training</h3>
<div class="outline-text-3" id="text-orgc0b6e15">
<p>
To avoid writing three training loops, we write a generic one that
takes a model instance. Since this is less about the training than the
network structure; you should feel free to skip this.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">loss_fn</span> = nn.CrossEntropyLoss()

<span style="color: #0000FF;">def</span> <span style="color: #006699;">run_training_loop</span>(model):
    <span style="color: #BA36A5;">num_args</span> = <span style="color: #006FE0;">len</span>(inspect.signature(model.forward).parameters)
    <span style="color: #BA36A5;">optimizer</span> = optim.Adam(model.parameters(), lr=1e-3)
    model.train()
    <span style="color: #BA36A5;">loss_history</span> = []
    <span style="color: #0000FF;">for</span> epoch <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(num_epochs):
        <span style="color: #BA36A5;">class_labels</span>, <span style="color: #BA36A5;">x_0</span>, <span style="color: #BA36A5;">x_1</span> = rand_dataset(num_samples)

        <span style="color: #0000FF;">if</span> num_args == 2:
            <span style="color: #BA36A5;">logits</span> = model(x_0, x_1)
        <span style="color: #0000FF;">elif</span> num_args == 1:
            <span style="color: #BA36A5;">logits</span> = model(x_0)
        <span style="color: #0000FF;">else</span>:
            <span style="color: #0000FF;">raise</span> <span style="color: #6434A3;">ValueError</span>(<span style="color: #008000;">"Model must have 1 or 2 arguments"</span>)
        <span style="color: #BA36A5;">loss</span> = loss_fn(logits, class_labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        <span style="color: #0000FF;">if</span> epoch % 100 == 0:
            loss_history.append((epoch,
                                 loss.item(),
                                 model.__class__.<span style="color: #006FE0;">__name__</span>))
            <span style="color: #006FE0;">print</span>(f<span style="color: #008000;">"Epoch </span>{epoch}<span style="color: #008000;"> loss: </span>{loss.item()}<span style="color: #008000;">"</span>)
    <span style="color: #0000FF;">return</span> loss_history, model
</pre>
</div>
</div>
</div>

<div id="outline-container-orga1d7bfb" class="outline-3">
<h3 id="orga1d7bfb">Run the training loop</h3>
<div class="outline-text-3" id="text-orga1d7bfb">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">loss_history_uni</span>, <span style="color: #BA36A5;">model_uni</span> = run_training_loop(model_uni)
<span style="color: #BA36A5;">loss_history_cat</span>, <span style="color: #BA36A5;">model_cat</span> = run_training_loop(model_cat)
<span style="color: #BA36A5;">loss_history_multi</span>, <span style="color: #BA36A5;">model_multi</span> = run_training_loop(model_multi)

<span style="color: #BA36A5;">loss_uni_df</span> = pd.DataFrame(loss_history_uni, columns=[<span style="color: #008000;">'epoch'</span>, <span style="color: #008000;">'loss'</span>, <span style="color: #008000;">'model'</span>])
<span style="color: #BA36A5;">loss_cat_df</span> = pd.DataFrame(loss_history_cat, columns=[<span style="color: #008000;">'epoch'</span>, <span style="color: #008000;">'loss'</span>, <span style="color: #008000;">'model'</span>])
<span style="color: #BA36A5;">loss_multi_df</span> = pd.DataFrame(loss_history_multi, columns=[<span style="color: #008000;">'epoch'</span>, <span style="color: #008000;">'loss'</span>, <span style="color: #008000;">'model'</span>])

<span style="color: #BA36A5;">loss_df</span> = pd.concat([loss_multi_df, loss_uni_df, loss_cat_df])
loss_df.to_csv(loss_csv, index=<span style="color: #D0372D;">False</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-orga0b6d68" class="outline-3">
<h3 id="orga0b6d68">Visualise the training results</h3>
<div class="outline-text-3" id="text-orga0b6d68">
<p>
Figure <a href="#orgbb38339">1</a> shows the training curves for each of the
models considered. Obviously, this isn't as convincing as a separate
validation set, but it seems reasonable based on this to think that
the multimodal models are preforming on par with each other and that
both are doing better than the unimodal one<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>.
</p>


<div id="orgbb38339" class="figure">
<p><img src="./example-2024-02-08/loss.png" alt="loss.png" width="700px" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #BA36A5;">loss_df</span> = pd.read_csv(loss_csv)
<span style="color: #BA36A5;">loss_p9</span> = nn_plot.plot_loss_curve(loss_df)
loss_p9.save(loss_png, height = 5.8, width = 8.2)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgab046df" class="outline-3">
<h3 id="orgab046df">Discussion</h3>
<div class="outline-text-3" id="text-orgab046df">
<p>
Gated multimodal units provide a clean way to combine data from
multiple sources provided they can be mapped into a shared space. In
this simulation study, the GMU-based model is comparable to
concatenation, but for such a simple task that is not wildly
surprising. Part of the motivation for the GMU is that it will keep
the number of parameters smaller at higher dimensions, so I am not
surprised there are not substantial benefits in this small example.
Not surprisingly, the multimodal models do better than the univariate
one.
</p>
</div>

<div id="outline-container-org13adcb0" class="outline-4">
<h4 id="org13adcb0">Thanks</h4>
<div class="outline-text-4" id="text-org13adcb0">
<p>
Thanks to <a href="https://www.svi.edu.au/researchers/dr-chun-fung-jackson-kwok/">Jackson Kwok</a> for helpful comments on a draft of this.
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
This is perhaps a more artificial example than is ideal, but it is what they presented in the paper and it does provide an example of modes having different amounts of information.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
I have been warned that making these sort of assumptions is fraught with danger, but since the main point of interest is the GMU architecture I'll be bold.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Alexander E. Zarebski</p>
<p class="date">Created: 2025-02-21 Fri 17:51</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
