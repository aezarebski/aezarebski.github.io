<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-21 Fri 17:51 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AANN 28/12/2024</title>
<meta name="author" content="Alexander E. Zarebski" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="icon" type="image/png" href="../../resources/nicemacs-favicon.png">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/milligram/1.4.1/milligram.css">
<link rel="stylesheet" href="../../microgram.css">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AANN 28/12/2024</h1>
<div class="nav-buttons">
  <a class="button button-outline nav-button" href="./post-2024-06-06.html">Previous</a>
  <a class="button button-outline nav-button" href="../../index.html">Home</a>
  <a class="button button-outline nav-button" href="./index.html">Index</a>
  <a class="button button-outline nav-button" href="./post-2025-01-18.html">Next</a>
</div>

<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd018447">Real NVP: Normalizing flows</a>
<ul>
<li><a href="#orgd05699a">Overview</a></li>
<li><a href="#org6ebbb1e">Normalizing flows</a></li>
<li><a href="#sec:implementation">Network implementation</a></li>
<li><a href="#org9586b88">Training</a></li>
<li><a href="#org6a1e115">Example</a></li>
<li><a href="#org27593f8">Discussion</a></li>
<li><a href="#org3a1154d">Thanks</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgd018447" class="outline-2">
<h2 id="orgd018447">Real NVP: Normalizing flows</h2>
<div class="outline-text-2" id="text-orgd018447">

<div id="org2ea3ec1" class="figure">
<p><img src="./example-2024-12-28/cover-image.webp" alt="cover-image.webp" width="400px" />
</p>
</div>
</div>

<div id="outline-container-orgd05699a" class="outline-3">
<h3 id="orgd05699a">Overview</h3>
<div class="outline-text-3" id="text-orgd05699a">
<p>
In this example we will implement the <a href="https://arxiv.org/abs/1605.08803">real NVP</a>, which is a
<i>normalizing flow</i> model. I will follow the description given in
Chapter 18 of <a href="https://www.bishopbook.com/">Bishop and Bishop (2024)</a>. The Bishops did a great job
describing the mathematics, I want to demonstrate how it might be
implemented in PyTorch.
</p>

<p>
The goal of normalizing flows is to create an invertible mapping
between samples from a <i>target</i> distribution and a <i>base</i>
distribution. As a bonus, we get an approximation of the target's PDF.
The target distribution will typically be something expensive to
sample from, and we may not be able to evaluate its density. The base
distribution will be a convenient distribution, typically a
multivariate normal (hence the "normalizing" part).
</p>

<p>
The <i>real NVP</i> (short for "real-valued non-volume-preserving") is an
invertible neural network that can be used as a normalizing flow. This
network is described clearly by Bishop and Bishop (2024) in \(\S\)
18.1, so I won't go into the mathematical details here, instead, I
will present an implementation in PyTorch with a simple example. You
can find the model implementation <a href="./example-2024-12-28/realNV.py">here</a>, and training script <a href="./example-2024-12-28/train.py">here</a>, and
a snakefile which coordinates the computation <a href="./example-2024-12-28/snakefile">here</a>.
</p>
</div>
</div>

<div id="outline-container-org6ebbb1e" class="outline-3">
<h3 id="org6ebbb1e">Normalizing flows</h3>
<div class="outline-text-3" id="text-org6ebbb1e">
<p>
Consider a random variable \(X\) with an unknown target density
\(p_X(x)\). Typically, \(X\) represents high-dimensional or complex
data, such as the pixel values in an image, making it challenging to
model or manipulate directly. Normalizing flows let us work with \(X\)
indirectly, through a more convenient random variable \(Z\). The
random variable \(Z\) comes from a simpler base distribution
\(p_Z(z)\), e.g., a multivariate Gaussian. The goal is to construct an
invertible transformation \(f\) that relates \(X\) and \(Z\) through
\(X = f(Z)\). The inverse of \(f\), denoted \(g = f^{-1}\), allows us
to map data from the \(X\)-space back to the \(Z\)-space.
</p>

<p>
The function \(g\) lets us calculate the density \(p_X(x)\) in terms
of the \(p_Z(z)\) via the change of variables formula:
</p>

<p>
\[
p_X(x) = p_Z(g(x)) \left|\det \mathbf{J}(x)\right|,
\]
</p>

<p>
where \(\mathbf{J}(x)\) is the Jacobian of the inverse transformation
\(g\). In the Jacobian, the entry \((i, j)\) represents the partial
derivative \(\frac{\partial g_i(x)}{\partial x_j}\). Having \(f\) and
\(g\) lets us do two important things: sample from \(p_X(x)\), by
sampling from \(p_Z(z)\) and applying the transformation \(f\); and
evaluate \(p_X(x)\) using the formula above.
</p>

<p>
Normalizing flows approximate \(f\) with a neural network. The network
architecture must be designed so that \(f\) is invertible, the
Jacobian is computationally efficient, and the \(g\) is easy to
compute. Real NVP (Real-valued Non-Volume Preserving) networks are an
example of such an architecture.
</p>
</div>
</div>

<div id="outline-container-sec:implementation" class="outline-3">
<h3 id="sec:implementation">Network implementation</h3>
<div class="outline-text-3" id="text-sec:implementation">
<p>
I implemented the real NVP in the class <code>RealNVP</code>. This extends the
PyTorch <code>Module</code> class and adds methods for running the network in
reverse, <code>reverse</code>, and computing the base density and determinant of
the Jacobian. These methods are helpful when training and using the
network.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">RealNVP</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>, dim_a, dim_b, num_flows):
        <span style="color: #006FE0;">super</span>(RealNVP, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">assert</span> (num_flows &gt; 0) <span style="color: #0000FF;">and</span> (
            num_flows % 2 == 0
        ), <span style="color: #008000;">"num_flows must be a positive even integer"</span>
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">dim_a</span> = dim_a
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">dim_b</span> = dim_b
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">num_flows</span> = num_flows
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">flows</span> = nn.ModuleList([RealNVPLayer(dim_a, dim_b) <span style="color: #0000FF;">for</span> _ <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(num_flows)])

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #0000FF;">for</span> f <span style="color: #0000FF;">in</span> <span style="color: #0000FF;">self</span>.flows:
            <span style="color: #BA36A5;">x</span> = f(x)
            <span style="color: #BA36A5;">x</span> = torch.cat([x[:, <span style="color: #0000FF;">self</span>.dim_a :], x[:, : <span style="color: #0000FF;">self</span>.dim_a]], dim=1)
        <span style="color: #0000FF;">return</span> x

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">ln_base_pdf</span>(<span style="color: #0000FF;">self</span>, z):
        <span style="color: #0000FF;">return</span> -0.5 * torch.<span style="color: #006FE0;">sum</span>(z**2, dim=1)

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">ln_det_jacobian</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #BA36A5;">ldjs</span> = torch.zeros(x.size(0))
        <span style="color: #0000FF;">for</span> f <span style="color: #0000FF;">in</span> <span style="color: #0000FF;">self</span>.flows:
            <span style="color: #BA36A5;">ldjs</span> += f.ln_det_jacobian(x)
            <span style="color: #BA36A5;">x</span> = f(x)
            <span style="color: #BA36A5;">x</span> = torch.cat([x[:, <span style="color: #0000FF;">self</span>.dim_a :], x[:, : <span style="color: #0000FF;">self</span>.dim_a]], dim=1)
        <span style="color: #0000FF;">return</span> ldjs

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">reverse</span>(<span style="color: #0000FF;">self</span>, z):
        <span style="color: #0000FF;">for</span> f <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">reversed</span>(<span style="color: #0000FF;">self</span>.flows):
            <span style="color: #BA36A5;">z</span> = torch.cat([z[:, <span style="color: #0000FF;">self</span>.dim_a :], z[:, : <span style="color: #0000FF;">self</span>.dim_a]], dim=1)
            <span style="color: #BA36A5;">z</span> = f.reverse(z)
        <span style="color: #0000FF;">return</span> z

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">reverse_with_intermediates</span>(<span style="color: #0000FF;">self</span>, z):
        <span style="color: #BA36A5;">intermediates</span> = []
        <span style="color: #0000FF;">for</span> f <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">reversed</span>(<span style="color: #0000FF;">self</span>.flows):
            <span style="color: #BA36A5;">z</span> = torch.cat([z[:, <span style="color: #0000FF;">self</span>.dim_a :], z[:, : <span style="color: #0000FF;">self</span>.dim_a]], dim=1)
            <span style="color: #BA36A5;">z</span> = f.reverse(z)
            intermediates.append(z.detach().numpy())
        <span style="color: #0000FF;">return</span> intermediates
</pre>
</div>

<p>
Instances of <code>RealNVP</code> use a list of <code>RealNVPLayer</code> objects to
implement the actual transformation of the data. The determinant of
the Jacobian is computed at the layer level and aggregated by
<code>RealNVP</code>.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">RealNVPLayer</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>, dim_a, dim_b):
        <span style="color: #006FE0;">super</span>(RealNVPLayer, <span style="color: #0000FF;">self</span>).__init__()
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">dim_a</span> = dim_a
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">dim_b</span> = dim_b
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">s_nn</span> = nn.Sequential(
            nn.Linear(dim_a, 64), nn.LeakyReLU(), nn.Linear(64, dim_b), nn.LeakyReLU()
        )
        <span style="color: #0000FF;">self</span>.<span style="color: #BA36A5;">b_bb</span> = nn.Sequential(
            nn.Linear(dim_a, 64), nn.LeakyReLU(), nn.Linear(64, dim_b), nn.LeakyReLU()
        )

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #BA36A5;">x_a</span> = x[:, : <span style="color: #0000FF;">self</span>.dim_a]
        <span style="color: #BA36A5;">x_b</span> = x[:, <span style="color: #0000FF;">self</span>.dim_a :]
        <span style="color: #BA36A5;">z_a</span> = x_a
        <span style="color: #BA36A5;">z_b</span> = torch.exp(-<span style="color: #0000FF;">self</span>.s_nn(z_a)) * (x_b - <span style="color: #0000FF;">self</span>.b_bb(z_a))
        <span style="color: #BA36A5;">z</span> = torch.cat([z_a, z_b], dim=1)
        <span style="color: #0000FF;">return</span> z

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">ln_det_jacobian</span>(<span style="color: #0000FF;">self</span>, x):
        <span style="color: #BA36A5;">x_a</span> = x[:, : <span style="color: #0000FF;">self</span>.dim_a]
        <span style="color: #BA36A5;">s</span> = <span style="color: #0000FF;">self</span>.s_nn(x_a)
        <span style="color: #0000FF;">return</span> torch.<span style="color: #006FE0;">sum</span>(-s, dim=1)

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">reverse</span>(<span style="color: #0000FF;">self</span>, z):
        <span style="color: #BA36A5;">z_a</span> = z[:, : <span style="color: #0000FF;">self</span>.dim_a]
        <span style="color: #BA36A5;">z_b</span> = z[:, <span style="color: #0000FF;">self</span>.dim_a :]
        <span style="color: #BA36A5;">x_a</span> = z_a
        <span style="color: #BA36A5;">x_b</span> = torch.exp(<span style="color: #0000FF;">self</span>.s_nn(z_a)) * z_b + <span style="color: #0000FF;">self</span>.b_bb(z_a)
        <span style="color: #BA36A5;">x</span> = torch.cat([x_a, x_b], dim=1)
        <span style="color: #0000FF;">return</span> x
</pre>
</div>
</div>
</div>

<div id="outline-container-org9586b88" class="outline-3">
<h3 id="org9586b88">Training</h3>
<div class="outline-text-3" id="text-org9586b88">
<p>
Training minimises the negative log-likelihood of the training data.
The loss function is described by the following class:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">class</span> <span style="color: #6434A3;">NegLogLikelihoodLoss</span>(nn.Module):
    <span style="color: #0000FF;">def</span> <span style="color: #006699;">__init__</span>(<span style="color: #0000FF;">self</span>):
        <span style="color: #006FE0;">super</span>(NegLogLikelihoodLoss, <span style="color: #0000FF;">self</span>).__init__()

    <span style="color: #0000FF;">def</span> <span style="color: #006699;">forward</span>(<span style="color: #0000FF;">self</span>, x, flow):
        <span style="color: #BA36A5;">z</span> = flow(x)
        <span style="color: #BA36A5;">ln_base_pdf</span> = flow.ln_base_pdf(z)
        <span style="color: #BA36A5;">ln_det_jacobian</span> = flow.ln_det_jacobian(x)
        <span style="color: #0000FF;">return</span> -torch.mean(ln_base_pdf + ln_det_jacobian)
</pre>
</div>
</div>
</div>

<div id="outline-container-org6a1e115" class="outline-3">
<h3 id="org6a1e115">Example</h3>
<div class="outline-text-3" id="text-org6a1e115">
<p>
In the example, I used a standard normal distribution as the base
distribution and a (uniform) mixture of two normal distributions as
the target. The components of the target distribution where bivariate
normal distributions with mean vectors \((-2, -2)\) and \((2, 2)\) and the
identity as a covariance matrix. I trained the real NVP network with
16 layers (8 pairs with the variables flipped each time).
</p>

<p>
To train this network I used Adam with default settings, a learning
rate of \(2\times 10^{-4}\) and mini-batches of size 64 for 1,000
epochs. The training data consisted of 1,000 samples from the target
distribution.
</p>

<p>
Figure <a href="#orgdb3d762">1</a> shows the transformation that a sample from
the base distribution undergoes when passing through the trained
network. The plots show the distribution of some test data as it
passes through each of the pairs of layers of the network. The final
plot shows the training data in red. We can see that by the end of the
network the samples have a similar distribution to the training data.
</p>


<div id="orgdb3d762" class="figure">
<p><img src="./example-2024-12-28/results-figure.png" alt="results-figure.png" width="700px" />
</p>
</div>
</div>
</div>

<div id="outline-container-org27593f8" class="outline-3">
<h3 id="org27593f8">Discussion</h3>
<div class="outline-text-3" id="text-org27593f8">
<p>
I've wanted to do a post on normalizing flows for a long time; the
ability to easily map between distributions seems too good to be true.
The implementation of this network is a little bit more mathematically
involved than what has been seen in previous posts, but I was
satisfied with how it came together in the end. I couldn't see a
particularly nice way to assess the trained model, but there are a few
packages for normalizing flows, so maybe there are some nice ideas in
there that I could use.
</p>

<p>
In a follow up post, I want to extend this to get something along the
lines of BayesFlow <a href="https://ieeexplore.ieee.org/document/9298920">1</a>, <a href="https://arxiv.org/abs/2306.16015">2</a>. Maybe the availability of training data will
be a problem for real problems. I'm mainly interested in the
application to amortized inference though, and in that case some sort
of emulation might help.
</p>
</div>
</div>

<div id="outline-container-org3a1154d" class="outline-3">
<h3 id="org3a1154d">Thanks</h3>
<div class="outline-text-3" id="text-org3a1154d">
<p>
Thanks to <a href="https://www.svi.edu.au/researchers/dr-chun-fung-jackson-kwok/">Jackson Kwok</a>, and <a href="https://www.liamhodgkinson.com/">Liam Hodgkinson</a> for helpful comments on a
draft of this post.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alexander E. Zarebski</p>
<p class="date">Created: 2025-02-21 Fri 17:51</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
