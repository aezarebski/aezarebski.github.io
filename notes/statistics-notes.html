<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-30 Sat 14:32 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>statistics-aez-notes</title>
<meta name="author" content="Alexander E. Zarebski" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link id="stylesheet" rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">statistics-aez-notes</h1>
<p>
<a href="../index.html">Home</a>
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge13001b">Statistics notes</a>
<ul>
<li><a href="#orge379895">Fisher information</a></li>
<li><a href="#org3ee315c">Guassian approximation to posterior</a></li>
<li><a href="#org6c9921b">Is it statistically significant</a></li>
<li><a href="#org05dd2fe">Empirical Distribution Function</a></li>
<li><a href="#org3a51c1f">The bootstrap</a></li>
<li><a href="#org95eb172">MCMC</a></li>
<li><a href="#orga8dfc7e">Linear regression</a></li>
<li><a href="#org6cff123">Profile likelihood function</a></li>
<li><a href="#org36a91a8"><span class="todo TODO">TODO</span> Student's <i>t</i>-test</a></li>
<li><a href="#org6173c85"><span class="todo TODO">TODO</span> Wilcoxon signed-rank test</a></li>
<li><a href="#org2bea59f"><span class="todo TODO">TODO</span> Mann–Whitney <i>U</i> test</a></li>
<li><a href="#orged1a01d"><span class="todo TODO">TODO</span> Kruskal–Wallis <i>H</i> test</a></li>
<li><a href="#orgd5c927d">Contingency tables</a></li>
<li><a href="#orgc6c20f6">Linear Mixed-effects Models</a></li>
<li><a href="#org7d1d162">Moment closure for CTMC processes</a></li>
<li><a href="#org9323946">The Law of the unconscious statistician</a></li>
<li><a href="#orgc7fd0c1"><span class="todo TODO">TODO</span> Bland-Altman plot</a></li>
<li><a href="#orgd4aa852">Further reading</a></li>
</ul>
</li>
<li><a href="#org4761b21">Machine learning notes</a>
<ul>
<li><a href="#org4706dbb">Using neural networks to solve differential equations</a></li>
<li><a href="#org8e42416">Initialisation</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orge13001b" class="outline-2">
<h2 id="orge13001b">Statistics notes</h2>
<div class="outline-text-2" id="text-orge13001b">
</div>
<div id="outline-container-orge379895" class="outline-3">
<h3 id="orge379895">Fisher information</h3>
<div class="outline-text-3" id="text-orge379895">
<p>
For a random variable, \(X\) with distrtibution \(f(X;\theta)\) the <i>score
function</i> is
</p>

<p>
\[
s(X; \theta) = \frac{\partial}{\partial\theta} \log f(X:\theta).
\]
</p>

<p>
Under some mild assumptions, the mean value of \(s(X; \theta)\) (when viewed as
a function of \(X\)) is zero.
</p>

<p>
The <i>Fisher information</i> is defined as the variance of the score and denoted \(\mathcal{I}(\theta)\):
</p>

<p>
\[
\mathcal{I}(\theta) = \mathbb{E}\left[ \left(\frac{\partial}{\partial\theta} \log f(X:\theta) \right)^{2} \right].
\]
</p>

<p>
It can be shown that the Fisher information is equal to
</p>

<p>
\[
-\mathbb{E}\left[ \frac{\partial^{2}}{\partial\theta^{2}} \log f(X:\theta) \right]
\]
</p>

<p>
which can be more convenient to work with.
</p>
</div>
</div>

<div id="outline-container-org3ee315c" class="outline-3">
<h3 id="org3ee315c">Guassian approximation to posterior</h3>
<div class="outline-text-3" id="text-org3ee315c">
<p>
Consider a posterior distribution where the mode is at \(\theta^{*}\) and the
Hessian matrix at this point is \(h''(\theta^{*})\). If we consider a
multivariate Gaussian density, we can see that it will have its mode at the
mean, \(\mu\), and, assuming a covariance matrix, \(\Sigma\), the density will
have a Hessian of \(-\Sigma^{-1}\) at the mode. This motivates the Guassian
approximation at the mode of the posterior. Also see <a href="#orge379895">Fisher information</a>.
</p>
</div>
</div>

<div id="outline-container-org6c9921b" class="outline-3">
<h3 id="org6c9921b">Is it statistically significant</h3>
<div class="outline-text-3" id="text-org6c9921b">
<p>
Here is a table of values for testing whether the a binomial sample differs from
fair trials at a significance level of 0.05.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Trials</th>
<th scope="col" class="org-right">Lower</th>
<th scope="col" class="org-right">Upper</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">20</td>
<td class="org-right">6</td>
<td class="org-right">14</td>
</tr>

<tr>
<td class="org-right">40</td>
<td class="org-right">14</td>
<td class="org-right">26</td>
</tr>

<tr>
<td class="org-right">60</td>
<td class="org-right">22</td>
<td class="org-right">38</td>
</tr>

<tr>
<td class="org-right">80</td>
<td class="org-right">31</td>
<td class="org-right">49</td>
</tr>

<tr>
<td class="org-right">100</td>
<td class="org-right">40</td>
<td class="org-right">60</td>
</tr>

<tr>
<td class="org-right">200</td>
<td class="org-right">86</td>
<td class="org-right">114</td>
</tr>

<tr>
<td class="org-right">400</td>
<td class="org-right">180</td>
<td class="org-right">220</td>
</tr>

<tr>
<td class="org-right">600</td>
<td class="org-right">276</td>
<td class="org-right">324</td>
</tr>

<tr>
<td class="org-right">800</td>
<td class="org-right">372</td>
<td class="org-right">428</td>
</tr>

<tr>
<td class="org-right">1000</td>
<td class="org-right">469</td>
<td class="org-right">531</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org05dd2fe" class="outline-3">
<h3 id="org05dd2fe">Empirical Distribution Function</h3>
<div class="outline-text-3" id="text-org05dd2fe">
<p>
Let \(X_1,\dots,X_n\sim F\) IID on \(\mathbb{R}\), then the empirical
distribution function (EDF) is
</p>

<p>
\[
\hat{F}_n(x) = \frac{1}{n}\sum I(X_i \leq x)
\]
</p>

<p>
where \(I\) is the indicator function. Linearity shows
</p>

<p>
\[
\mathbb{E}\hat{F}_n(x) = F(x)
\]
</p>

<p>
and basic properties of variance and the Bernoulli distribution shows
</p>

<p>
\[
\mathbb{V}\hat{F}_n(x) = \frac{1}{n} F(x) (1 - F(x)).
\]
</p>

<p>
The EDF converges to the CDF in probability (which can be observed from Markov's
inequality.) The DKW inequality bounds this convergence which allows for the
construction of confidence bounds on the EDF. A functional \(T : F \mapsto
\theta\) is called a statistical functional. The <i>plug-in estimator</i> of
\(\theta\) is simply \(T(\hat{F}_n)\).
</p>
</div>
</div>

<div id="outline-container-org3a51c1f" class="outline-3">
<h3 id="org3a51c1f">The bootstrap</h3>
<div class="outline-text-3" id="text-org3a51c1f">
<p>
The bootstrap is a simulation based method to estimate standard errors and
confidence intervals. Consider a statistic \(T\) which is a function of a sample
of \(X_i\sim F\); we want to compute \(\mathbb{V}_F(T)\). The subscript \(F\) is
to indicate that this is with respect to the distribution \(F\). Let \(\hat{F}\)
be the empirical distribution (EDF) of the \(X_i\). The bootstrap uses
\(\mathbb{V}_F(T)\) to approximate \(\mathbb{V}_{\hat{F}}(T)\) which is then
itself estimated via simulation.
</p>

<p>
The simulation typically is just sampling from the EDF with replacement, and we
can always run more simulation to get an arbitrarily good approximation of
\(\mathbb{V}_{\hat{F}}(T)\). The real source of error is how well \(\hat{F}\)
approximate \(F\).
</p>

<p>
In the <i>parametric</i> bootstrap, rather than sampling from the EDF of the data, a
sample is generated from the parametric distribution parameterised by the
estimated parameter value.
</p>
</div>
</div>

<div id="outline-container-org95eb172" class="outline-3">
<h3 id="org95eb172">MCMC</h3>
<div class="outline-text-3" id="text-org95eb172">
<p>
See Chib and Greenberg (1995) for a timeless introduction to the
Metropolis-Hasting algorithm.
</p>
</div>

<div id="outline-container-org99599dd" class="outline-4">
<h4 id="org99599dd">Parameter transformation</h4>
<div class="outline-text-4" id="text-org99599dd">
<p>
You want to sample \(x\) which has density \(f\) but the MCMC samples are
butting up against the edge of the support. For example, we have \(X\sim
\text{Lognormal}(0,1)\).
</p>

<div class="org-src-container">
<pre class="src src-R"><span style="color: #3a81c3;">library</span><span style="color: #3a81c3;">(</span>mcmc<span style="color: #3a81c3;">)</span>
<span style="color: #3a81c3;">library</span><span style="color: #3a81c3;">(</span>coda<span style="color: #3a81c3;">)</span>
set.seed<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">)</span>

<span style="color: #6c3163; font-weight: bold;">mcmc_samples</span> <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> <span style="color: #3a81c3;">function</span><span style="color: #3a81c3;">(</span>obj<span style="color: #3a81c3;">)</span> <span style="color: #3a81c3;">{</span>
    as.mcmc<span style="color: #6c3163;">(</span>metrop<span style="color: #2d9574;">(</span>obj, <span style="color: #4e3163;">1</span>, <span style="color: #4e3163;">10000</span><span style="color: #2d9574;">)</span>$batch<span style="color: #6c3163;">)</span>
<span style="color: #3a81c3;">}</span>

<span style="color: #6c3163; font-weight: bold;">posterior</span> <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> <span style="color: #3a81c3;">function</span><span style="color: #3a81c3;">(</span>x<span style="color: #3a81c3;">)</span> <span style="color: #3a81c3;">{</span>
    dlnorm<span style="color: #6c3163;">(</span>x = x, meanlog = <span style="color: #4e3163;">0</span>, sdlog = <span style="color: #4e3163;">1</span>, log = <span style="color: #4e3163;">TRUE</span><span style="color: #6c3163;">)</span>
<span style="color: #3a81c3;">}</span>

x_samples <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> mcmc_samples<span style="color: #3a81c3;">(</span>posterior<span style="color: #3a81c3;">)</span>
</pre>
</div>

<p>
But the sampler keeps jumping into the negative numbers which is a problem. So
you might consider \(Y = log(X)\) as this takes values on the whole real line.
We need to derive the posterior distribution for this, \(g\). Note
</p>

<p>
\[
\left|g(y)dy\right| = \left|f(x)dx\right|,
\]
</p>

<p>
put another way,
</p>

<p>
\[
g(y) = f(x(y)) \left|\frac{dx(y)}{dy}\right|.
\]
</p>

<p>
So, if we let \(x = \exp(y)\), then the Jacobian is just \(\exp(y)\), and when we
take the logarithm to get the log-posterior this becomes just \(y\).
</p>

<div class="org-src-container">
<pre class="src src-R"><span style="color: #6c3163; font-weight: bold;">y_posterior</span> <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> <span style="color: #3a81c3;">function</span><span style="color: #3a81c3;">(</span>y<span style="color: #3a81c3;">)</span> <span style="color: #3a81c3;">{</span>
    posterior<span style="color: #6c3163;">(</span>exp<span style="color: #2d9574;">(</span>y<span style="color: #2d9574;">)</span><span style="color: #6c3163;">)</span> + y
<span style="color: #3a81c3;">}</span>

y_samples <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> mcmc_samples<span style="color: #3a81c3;">(</span>y_posterior<span style="color: #3a81c3;">)</span>
</pre>
</div>

<p>
Then a Q-Q plot of the logarithm of the \(X\) samples and the \(Y\) samples
suggests we have gotten this correct. The tails are slightly different because
the second sampler has been able to explore smaller and larger values more
efficiently.
</p>


<div id="org2d5bf84" class="figure">
<p><img src="../images/mcmc-transform-demo.png" alt="mcmc-transform-demo.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orga8dfc7e" class="outline-3">
<h3 id="orga8dfc7e">Linear regression</h3>
<div class="outline-text-3" id="text-orga8dfc7e">
</div>
<div id="outline-container-orgece8f09" class="outline-4">
<h4 id="orgece8f09">Linear regression with <code>lm</code></h4>
<div class="outline-text-4" id="text-orgece8f09">
<p>
The formulas used to specify a model in R use Wilkinson-Rogers notation.
Consider \(y = \alpha + \beta x + \epsilon\). Often estimators will assume that
the \(\epsilon\) are IID normal random variables. To test whether the
\(\epsilon\) are homoscedastic one might use the <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test">Breusch-Pagan test</a>. This is
available in R via <code>lmtest::bptest</code>. To test whether the errors follow a normal
distribution, a sensible first pass would be to generate a QQ-plot. But if a
formal method is required there is the <code>stats::shapiro.test</code> function for
normality.
</p>

<p>
Assuming that for the most part the assumptions appear to be valid, we might
dive deeper into the data. The <i>leverage</i> of a datum can be thought of as the
potential to influence parameters and can be calculated with <code>stats::hatvalues</code>.
However, high leverage is not necessarily a bad thing unless it is also an
outlier. One way to measure how plausible a measurement is to have arisen from
the model is by considering its standardised residual, <code>rstandard</code>.
</p>

<p>
Combining leverage and residual into a single measure, is the goal of the Cook's
distance which is one of the summaries produced by <code>plot.lm</code>. A rule of thumb is
that you want the Cook's distance to be not greater than \(4 N^{-1}\) for a
dataset of size \(N\).
</p>
</div>
</div>
</div>

<div id="outline-container-org6cff123" class="outline-3">
<h3 id="org6cff123">Profile likelihood function</h3>
<div class="outline-text-3" id="text-org6cff123">
<p>
The profile likelihood is a lower dimensional version of the likelihood
function. Consider a likelihood function \(\mathcal{L}(\theta)\) where \(\theta
= (\psi,\lambda)\) where \(\lambda\) are nuisance parameters. The <i>profile
likelihood</i> is
</p>

<p>
\[
\mathcal{L}_{\text{profile}}(\psi) := \max_{\lambda} \mathcal{L}(\psi,\lambda).
\]
</p>
</div>
</div>

<div id="outline-container-org36a91a8" class="outline-3">
<h3 id="org36a91a8"><span class="todo TODO">TODO</span> Student's <i>t</i>-test</h3>
<div class="outline-text-3" id="text-org36a91a8">
</div>
<div id="outline-container-orge5f8ae4" class="outline-4">
<h4 id="orge5f8ae4">Null is specific mean</h4>
<div class="outline-text-4" id="text-orge5f8ae4">
<div class="org-src-container">
<pre class="src src-R">t.test<span style="color: #3a81c3;">(</span>rnorm<span style="color: #6c3163;">(</span><span style="color: #4e3163;">100</span>, mean=<span style="color: #4e3163;">1</span><span style="color: #6c3163;">)</span>, mu = <span style="color: #4e3163;">0</span><span style="color: #3a81c3;">)</span>$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-org70d843b" class="outline-4">
<h4 id="org70d843b">Null is different mean (assumed equal variance)</h4>
<div class="outline-text-4" id="text-org70d843b">
<div class="org-src-container">
<pre class="src src-R">t.test<span style="color: #3a81c3;">(</span>rnorm<span style="color: #6c3163;">(</span><span style="color: #4e3163;">100</span>, mean = <span style="color: #4e3163;">0</span><span style="color: #6c3163;">)</span>, rnorm<span style="color: #6c3163;">(</span><span style="color: #4e3163;">100</span>, mean = <span style="color: #4e3163;">1</span><span style="color: #6c3163;">)</span>, var.equal = <span style="color: #4e3163;">TRUE</span><span style="color: #3a81c3;">)</span>$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf67f4de" class="outline-4">
<h4 id="orgf67f4de">Null is different mean (Welch)</h4>
<div class="outline-text-4" id="text-orgf67f4de">
<div class="org-src-container">
<pre class="src src-R">t.test<span style="color: #3a81c3;">(</span>rnorm<span style="color: #6c3163;">(</span><span style="color: #4e3163;">100</span>, mean = <span style="color: #4e3163;">0</span><span style="color: #6c3163;">)</span>, rnorm<span style="color: #6c3163;">(</span><span style="color: #4e3163;">100</span>, mean = <span style="color: #4e3163;">1</span>, sd = <span style="color: #4e3163;">2</span><span style="color: #6c3163;">)</span>, var.equal = <span style="color: #4e3163;">FALSE</span><span style="color: #3a81c3;">)</span>$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-org4a82aeb" class="outline-4">
<h4 id="org4a82aeb"><span class="todo TODO">TODO</span> Null is different mean but values are paired</h4>
</div>
</div>

<div id="outline-container-org6173c85" class="outline-3">
<h3 id="org6173c85"><span class="todo TODO">TODO</span> Wilcoxon signed-rank test</h3>
<div class="outline-text-3" id="text-org6173c85">
<p>
Roughly, it plays the role of a nonparametric paired <i>t</i>-test.
</p>
</div>
</div>

<div id="outline-container-org2bea59f" class="outline-3">
<h3 id="org2bea59f"><span class="todo TODO">TODO</span> Mann–Whitney <i>U</i> test</h3>
<div class="outline-text-3" id="text-org2bea59f">
<p>
Roughly, it plays the role of a nonparametric <i>t</i>-test.
</p>
</div>
</div>

<div id="outline-container-orged1a01d" class="outline-3">
<h3 id="orged1a01d"><span class="todo TODO">TODO</span> Kruskal–Wallis <i>H</i> test</h3>
<div class="outline-text-3" id="text-orged1a01d">
<p>
Roughly, it extends the <a href="#org2bea59f">Mann–Whitney <i>U</i> test</a> to more than two groups.
</p>
</div>
</div>

<div id="outline-container-orgd5c927d" class="outline-3">
<h3 id="orgd5c927d">Contingency tables</h3>
<div class="outline-text-3" id="text-orgd5c927d">
</div>
<div id="outline-container-orgbb809df" class="outline-4">
<h4 id="orgbb809df">An example in R</h4>
<div class="outline-text-4" id="text-orgbb809df">
<p>
A contingency table counts the number of times that a particular combination of
catagorical variables occur. For example, we can simulate a data set of
catagorical variables as follows
</p>

<div class="org-src-container">
<pre class="src src-R">set.seed<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">)</span>
x <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> sample<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span>:<span style="color: #4e3163;">3</span>, <span style="color: #4e3163;">1000</span>, replace = <span style="color: #4e3163;">TRUE</span><span style="color: #3a81c3;">)</span>
y <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> sample<span style="color: #3a81c3;">(</span>letters<span style="color: #6c3163;">[</span><span style="color: #4e3163;">1</span>:<span style="color: #4e3163;">3</span><span style="color: #6c3163;">]</span>, <span style="color: #4e3163;">1000</span>, replace = <span style="color: #4e3163;">TRUE</span><span style="color: #3a81c3;">)</span>
df <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> data.frame<span style="color: #3a81c3;">(</span>x, y<span style="color: #3a81c3;">)</span>
</pre>
</div>

<p>
Then we can create a contingency table from this with the <code>xtabs</code> function.
</p>

<div class="org-src-container">
<pre class="src src-R">tab <span style="color: #ba2f59; font-weight: bold;">&lt;-</span> xtabs<span style="color: #3a81c3;">(</span>~ x + y, data = df<span style="color: #3a81c3;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-R">&gt; print<span style="color: #3a81c3;">(</span>tab<span style="color: #3a81c3;">)</span>
   y
x     a   b   c
  <span style="color: #4e3163;">1</span> <span style="color: #4e3163;">131</span> <span style="color: #4e3163;">109</span> <span style="color: #4e3163;">111</span>
  <span style="color: #4e3163;">2</span> <span style="color: #4e3163;">100</span> <span style="color: #4e3163;">122</span> <span style="color: #4e3163;">117</span>
  <span style="color: #4e3163;">3</span> <span style="color: #4e3163;">100</span>  <span style="color: #4e3163;">99</span> <span style="color: #4e3163;">111</span>
</pre>
</div>

<p>
The null hypothesis test that we are interested in is that there is no
association between the catagorical variables <code>x</code> and <code>y</code>. If each variable was
binary we could use Fisher's exact test, but since there are more, and there are
\(\geq 10\) observations in each catagory a $&chi;<sup>2</sup>$-test is acceptable.
</p>

<div class="org-src-container">
<pre class="src src-R">&gt; print<span style="color: #3a81c3;">(</span>chisq.test<span style="color: #6c3163;">(</span>tab<span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>

  Pearson<span style="color: #2d9574;">'s Chi-squared test</span>

<span style="color: #2d9574;">data:  tab</span>
<span style="color: #2d9574;">X-squared = 5.6178, df = 4, p-value = 0.2296</span>

</pre>
</div>

<p>
Since the variables were simulated independently the $p$-value is, not
surprisingly, large enough that it would not be considered significant.
</p>
</div>
</div>

<div id="outline-container-org7c6faf6" class="outline-4">
<h4 id="org7c6faf6">What actually happens</h4>
<div class="outline-text-4" id="text-org7c6faf6">
<p>
Let \(f_{ij}\) be the value in the \(ij\)-th cell of the contingency table and
\(e_{ij}\) the expected value assuming that the observations are distributed such
that the catagorical variables are independent. Consider the following
statistic:
</p>

<p>
\[
\sum_{(i,j)} \frac{ (f_{ij} - e_{ij})^2 }{ e_{ij} }
\]
</p>

<p>
This statistic has a \(\chi^2\)-distribution with \((I-1)(J-1)\) degrees of
freedom where \(I\) and \(J\) are the number of distinct values each variable
takes.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc6c20f6" class="outline-3">
<h3 id="orgc6c20f6">Linear Mixed-effects Models</h3>
<div class="outline-text-3" id="text-orgc6c20f6">
<p>
The Laird-Ware form of a linear mixed effect model (LMM) for the \(j\)th
observation in the \(i\)th group of measurements is as follows:
</p>

<p>
\[
Y_{ij} = \beta_1 + \sum_k \beta_k X_{kij} + \sum_{k} \delta_{ki} Z_{kij} + \epsilon_{ij}.
\]
</p>

<ul class="org-ul">
<li>the \(\beta_k\) are the <i>fixed effect</i> coefficients and the \(X_{kij}\) the
fixed effect covariates,</li>
<li>the \(\delta_k\) are the <i>random effect</i> coefficients and the \(Z_{kij}\) the
random effect covariates, it is important to note that while the \(beta_k\)
are treated as parameters to be estimated, the \(\delta_k\) are treated as
random variables and it is their distribution that is estimated.</li>
<li>the \(\epsilon_{ij}\) is a random variable.</li>
<li>the distribution of the random effect coefficients is a (zero-mean)
multivariate normal distribution parameterised by \(\psi\) and the random
noise \(\epsilon\) comes from a zero-mean normal distribution with variance
parameterised by \(\sigma^2\).</li>
</ul>
</div>

<div id="outline-container-orgfc11b1d" class="outline-4">
<h4 id="orgfc11b1d">Model derivation</h4>
<div class="outline-text-4" id="text-orgfc11b1d">
<p>
One way to go about deriving a LMM for a particular data set is to consider a
model at the individual level and then assume some random structure on the
parameters which varies at the group level. Expanding this out will lead to a
model in the Laird-Ware form. The random variables in the model at the group
level creates the random effects terms in the Laird-Ware form where the constant
parts of the parameters form the fixed effects.
</p>
</div>
</div>

<div id="outline-container-org86763eb" class="outline-4">
<h4 id="org86763eb">Model notation</h4>
<div class="outline-text-4" id="text-org86763eb">
<p>
The <code>lme4</code> package in R introduces some syntax for describing these models.
</p>

<ul class="org-ul">
<li><code>(expr | factor)</code> is used to indicate that the expression <code>expr</code> represents
random effects and that these values should be common across <code>factor</code>. By
default, this assumes that there are correlations between the random effects.</li>
<li><code>(expr || factor)</code> is another way to specify that the <code>expr</code> are random
effects, but assumes that they are uncorrelated.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7d1d162" class="outline-3">
<h3 id="org7d1d162">Moment closure for CTMC processes</h3>
<div class="outline-text-3" id="text-org7d1d162">
<p>
Consider a random process, \(X_t\) which is a CTMC on \(\mathbb{N}_0\) where the
only possible transitions from \(n\) are to \(n\pm 1\), potentially with an
absorbing state at zero. Supposing from state \(n\) the process moves to state
\(n+1\) at rate \(a_n\) and to state \(n-1\) at rate \(b_n\) the forward
equations for the distribution are
</p>

<p>
\[
\frac{d}{dt} p_n(t) = p_{n-1}(t)a_{n-1} - p_{n}(t)a_n + p_{n+1}(t)b_{n+1} - p_{n}(t)b_{n}.
\]
</p>

<p>
We multiple both sides of the equation through by \(n\) and sum in \(n\) over
\(\mathbb{N}_0\) to get everything in terms of expected values of functions of
\(X_t\).
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t] = \mathbb{E}[((X_t + 1) - X_t) a_{X_t}] + \mathbb{E}[((X_t - 1) - X_t) b_{X_t}]
\]
</p>

<p>
The "trick" is to note that you can write the first and third sums as \(\sum
(n+1) p_{n}(t) a_{n}\) and \(\sum (n-1) p_n b_n\). Similarly, for higher order
moments this generalises as
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \mathbb{E}[((X_t + 1)^k - X_t^k) a_{X_t}] + \mathbb{E}[((X_t - 1)^k - X_t^k) b_{X_t}].
\]
</p>

<p>
Recall the binomial formula,
</p>

<p>
\[
(x+y)^k = \sum_{i = 0}^{k} \binom{k}{i} x^i y^{k-i}.
\]
</p>

<p>
This can then be used to derive the differential equations for the moments.
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \sum_{i = 0}^{k-1} \binom{k}{i} \left(  \mathbb{E}[ X_t^i a_{X_t} ]  + (-1)^{k-i} \mathbb{E}[ X_t^i b_{X_t} ] \right)
\]
</p>

<p>
Note that the sums are from \(0\) to \(k-1\) but the binomial coefficients use
the full \(k\). Given this expression, any polynomial \(a_n\) and \(b_n\) are
candidates for the application of a moment closure, which essentially just
entails either truncating the cumulants, or assuming a distributional form which
expresses higher moments in terms of lower ones.
</p>
</div>
</div>

<div id="outline-container-org9323946" class="outline-3">
<h3 id="org9323946">The Law of the unconscious statistician</h3>
<div class="outline-text-3" id="text-org9323946">
<p>
This refers to the equation
</p>

<p>
\[
\mathbb{E}g(X) = \int g(x) f_{X}(x) dx.
\]
</p>

<p>
The name derives from the fact that this is so often treated as self-evident
rather than being viewed as a theorem.
</p>

<p>
This result is particularly useful when carrying out MCMC in a transformed
parameter space as described in the <a href="#org99599dd">notes on changing parameterisation</a> above.
</p>
</div>
</div>

<div id="outline-container-orgc7fd0c1" class="outline-3">
<h3 id="orgc7fd0c1"><span class="todo TODO">TODO</span> Bland-Altman plot</h3>
<div class="outline-text-3" id="text-orgc7fd0c1">
<p>
Given to ways to measure something, a Bland-Altman plot is a way to compare
them.
</p>
</div>
</div>

<div id="outline-container-orgd4aa852" class="outline-3">
<h3 id="orgd4aa852">Further reading</h3>
<div class="outline-text-3" id="text-orgd4aa852">
<ul class="org-ul">
<li><a href="https://pages.cs.wisc.edu/~tdw/files/cookbook-en.pdf">Probability and Statistics Cookbook</a> from <a href="http://matthias.vallentin.net/">Matthias Vallentin</a>.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4761b21" class="outline-2">
<h2 id="org4761b21">Machine learning notes</h2>
<div class="outline-text-2" id="text-org4761b21">
<blockquote>
<p>
The first rule of machine learning: Start without machine learning
</p>

<p>
— Eugene Yan (@eugeneyan) September 10, 2021
</p>
</blockquote>
</div>

<div id="outline-container-org4706dbb" class="outline-3">
<h3 id="org4706dbb">Using neural networks to solve differential equations</h3>
<div class="outline-text-3" id="text-org4706dbb">
<p>
Lagaris <i>et al</i> (1998) describe a method to solve differential equations
numerically with neural networks. The simplest example they use is the following
differential equation:
</p>

<p>
\[
\frac{d\Psi}{dx} + \left(x + \frac{1 + 3x^{2}}{1 + x + x^{3}}\right) \Psi = x^{3} + 2x+x^{2}\frac{1 +3x^{2}}{1 + x + x^{3}}
\]
</p>

<p>
with the initial condition that \(\Psi(0) = A\) for \(0\leq x\leq 1\). The
solution to this equation is available in closed form:
</p>

<p>
\[
\Psi = \frac{e^{-x^{2} / 2}}{1 + x + x^{3}} + x^{2}.
\]
</p>

<p>
The trial solution proposed is
</p>

<p>
\[
\Psi_{t} = A + x N(x,p)
\]
</p>

<p>
where \(N\) is a neural network parameterised by \(p\). This trial solution
satisfied the initial condition for all values of \(p\) which leads to an
unconstrained optimisation problem. The loss function is
</p>

<p>
\[
\text{loss}(p) = \sum_{i} \left\{ \frac{d\Psi_{t}}{dx_{i}} + \left(x_{i} + \frac{1 + 3x_{i}^{2}}{1 + x_{i} + x_{i}^{3}}\right) \Psi_{t} - \left[ x_{i}^{3} + 2x_{i}+x_{i}^{2}\frac{1 +3x_{i}^{2}}{1 + x_{i} + x_{i}^{3}} \right] \right\}^{2}
\]
</p>

<p>
for some set of trial points \(x_{i}\). There are closed forms available for
both the loss function and its gradient which opens the way for multiple
optimisation routines to be applied.
</p>

<p>
Figure <a href="#org27b6cfa">1</a> shows the result of less than a minute of training with
gradient descent with only ten test points.
</p>


<div id="org27b6cfa" class="figure">
<p><img src="./img/lagaris-1998.png" alt="lagaris-1998.png" width="900px" />
</p>
<p><span class="figure-number">Figure 1: </span>Replication of the first example problem from Lagaris <i>et al</i> (1998)</p>
</div>

<p>
The script that generated the image in Figure <a href="#org27b6cfa">1</a> is shown below.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #3a81c3;">import</span> tensorflow <span style="color: #3a81c3;">as</span> tf
<span style="color: #3a81c3;">import</span> numpy <span style="color: #3a81c3;">as</span> np
<span style="color: #3a81c3;">import</span> matplotlib.pyplot <span style="color: #3a81c3;">as</span> plt

np.random.seed<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">)</span>

<span style="color: #715ab1;">H</span> = <span style="color: #4e3163;">10</span>
<span style="color: #715ab1;">mesh_size</span> = <span style="color: #4e3163;">10</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">We need to select some points to train the model on and some to test it on.</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">It looks like you need to have this as a mutable value if you want to compute</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">gradients.</span>
<span style="color: #715ab1;">train_mesh</span> = tf.Variable<span style="color: #3a81c3;">(</span>np.linspace<span style="color: #6c3163;">(</span><span style="color: #4e3163;">0</span>, <span style="color: #4e3163;">1</span>, mesh_size<span style="color: #6c3163;">)</span>, dtype=np.float64<span style="color: #3a81c3;">)</span>
test_mesh = tf.Variable<span style="color: #3a81c3;">(</span>np.linspace<span style="color: #6c3163;">(</span><span style="color: #4e3163;">0</span>.<span style="color: #4e3163;">01</span>, <span style="color: #4e3163;">0</span>.<span style="color: #4e3163;">99</span>, mesh_size<span style="color: #6c3163;">)</span>, dtype=np.float64<span style="color: #3a81c3;">)</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">We need to select some points to plot the resulting approximation on.</span>
plt_mesh = np.linspace<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">0</span>, <span style="color: #4e3163;">2</span>, <span style="color: #4e3163;">100</span><span style="color: #3a81c3;">)</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">We need to record the solution at multiple points of the training loop to</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">test that it is infact converging so something sensible.</span>
psi_val_stash = <span style="color: #3a81c3;">[]</span>
iter_stash = <span style="color: #3a81c3;">[]</span>
loss_stash = <span style="color: #3a81c3;">[]</span>
vldt_stash = <span style="color: #3a81c3;">[]</span>

<span style="color: #ba2f59; font-weight: bold;">@tf.function</span>
<span style="color: #3a81c3;">def</span> <span style="color: #6c3163; font-weight: bold;">psi_truth</span><span style="color: #3a81c3;">(</span>x<span style="color: #3a81c3;">)</span>:
    <span style="color: #da8b55;">"""Solution of the differential equation."""</span>
    <span style="color: #3a81c3;">return</span> tf.exp<span style="color: #3a81c3;">(</span>-<span style="color: #4e3163;">0</span>.<span style="color: #4e3163;">5</span> * x**<span style="color: #4e3163;">2</span><span style="color: #3a81c3;">)</span> / <span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span> + x + x**<span style="color: #4e3163;">3</span><span style="color: #3a81c3;">)</span> + x**<span style="color: #4e3163;">2</span>


<span style="color: #3a81c3;">class</span> <span style="color: #ba2f59; font-weight: bold;">MyModel</span><span style="color: #3a81c3;">(</span>tf.Module<span style="color: #3a81c3;">)</span>:
    <span style="color: #da8b55;">"""This class respresents the approximation to the solution."""</span>

    <span style="color: #3a81c3;">def</span> <span style="color: #6c3163; font-weight: bold;">__init__</span><span style="color: #3a81c3;">(</span><span style="color: #3a81c3;">self</span>, psi_0, num_units, **kwargs<span style="color: #3a81c3;">)</span>:
        <span style="color: #da8b55;">"""Construct approximate solution with a hard-coded IC."""</span>
        <span style="color: #3a81c3;">super</span><span style="color: #3a81c3;">()</span>.__init__<span style="color: #3a81c3;">(</span>**kwargs<span style="color: #3a81c3;">)</span>
        <span style="color: #3a81c3;">self</span>.IC_A = psi_0
        <span style="color: #3a81c3;">self</span>.w = tf.Variable<span style="color: #3a81c3;">(</span>np.random.randn<span style="color: #6c3163;">(</span>num_units<span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>
        <span style="color: #3a81c3;">self</span>.u = tf.Variable<span style="color: #3a81c3;">(</span>np.random.randn<span style="color: #6c3163;">(</span>num_units<span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>
        <span style="color: #3a81c3;">self</span>.v = tf.Variable<span style="color: #3a81c3;">(</span>np.random.randn<span style="color: #6c3163;">(</span>num_units<span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>

    <span style="color: #3a81c3;">def</span> <span style="color: #6c3163; font-weight: bold;">__call__</span><span style="color: #3a81c3;">(</span><span style="color: #3a81c3;">self</span>, x<span style="color: #3a81c3;">)</span>:
        <span style="color: #da8b55;">"""Evaluate the approximation."""</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">matrix with one datum per row</span>
        l1 = tf.math.sigmoid<span style="color: #3a81c3;">(</span>tf.tensordot<span style="color: #6c3163;">(</span>x, <span style="color: #3a81c3;">self</span>.w, <span style="color: #4e3163;">0</span><span style="color: #6c3163;">)</span> + <span style="color: #3a81c3;">self</span>.u<span style="color: #3a81c3;">)</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">We have encoded the IC using the method from Lagaris et al (1998),</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">although there are other formulations that are used in contemporary</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">implementations. Attempting to use a 1-exp(-x) function to get the IC</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">constraint did not yield any improvement.</span>
        l2 = <span style="color: #3a81c3;">self</span>.IC_A + x * tf.tensordot<span style="color: #3a81c3;">(</span>l1, <span style="color: #3a81c3;">self</span>.v, <span style="color: #4e3163;">1</span><span style="color: #3a81c3;">)</span>
        <span style="color: #3a81c3;">return</span> l2


psi_model = MyModel<span style="color: #3a81c3;">(</span>tf.constant<span style="color: #6c3163;">(</span><span style="color: #4e3163;">1</span>.<span style="color: #4e3163;">0</span>, dtype=np.float64<span style="color: #6c3163;">)</span>, H<span style="color: #3a81c3;">)</span>

num_iters = <span style="color: #4e3163;">2000</span>
learning_rate = <span style="color: #4e3163;">0</span>.<span style="color: #4e3163;">050</span>
x = train_mesh


<span style="color: #3a81c3;">def</span> <span style="color: #6c3163; font-weight: bold;">loss_fn</span><span style="color: #3a81c3;">(</span>psi_dash, psi, x<span style="color: #3a81c3;">)</span>:
    <span style="color: #3a81c3;">return</span> tf.reduce_sum<span style="color: #3a81c3;">(</span><span style="color: #6c3163;">(</span>psi_trial_dash + <span style="color: #2d9574;">(</span>x + <span style="color: #67b11d;">(</span><span style="color: #4e3163;">1</span> + <span style="color: #4e3163;">3</span> * x**<span style="color: #4e3163;">2</span><span style="color: #67b11d;">)</span> / <span style="color: #67b11d;">(</span><span style="color: #4e3163;">1</span> + x + x**<span style="color: #4e3163;">3</span><span style="color: #67b11d;">)</span><span style="color: #2d9574;">)</span> * psi_trial - <span style="color: #2d9574;">(</span>x**<span style="color: #4e3163;">3</span> + <span style="color: #4e3163;">2</span> * x + x**<span style="color: #4e3163;">2</span> * <span style="color: #67b11d;">(</span><span style="color: #4e3163;">1</span> + <span style="color: #4e3163;">3</span> * x**<span style="color: #4e3163;">2</span><span style="color: #67b11d;">)</span> / <span style="color: #67b11d;">(</span><span style="color: #4e3163;">1</span> + x + x**<span style="color: #4e3163;">3</span><span style="color: #67b11d;">)</span><span style="color: #2d9574;">)</span><span style="color: #6c3163;">)</span>**<span style="color: #4e3163;">2</span><span style="color: #3a81c3;">)</span>


<span style="color: #3a81c3;">for</span> iter_num <span style="color: #3a81c3;">in</span> <span style="color: #3a81c3;">range</span><span style="color: #3a81c3;">(</span>num_iters<span style="color: #3a81c3;">)</span>:
    <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">The gradient evaluations need to be within to the loop for TF to</span>
    <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">understand how they work.</span>
    <span style="color: #3a81c3;">with</span> tf.GradientTape<span style="color: #3a81c3;">()</span> <span style="color: #3a81c3;">as</span> t2:
        <span style="color: #3a81c3;">with</span> tf.GradientTape<span style="color: #3a81c3;">()</span> <span style="color: #3a81c3;">as</span> t1:
            psi_trial = psi_model<span style="color: #3a81c3;">(</span>x<span style="color: #3a81c3;">)</span>
        psi_trial_dash = t1.gradient<span style="color: #3a81c3;">(</span>psi_trial, x<span style="color: #3a81c3;">)</span>
        <span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">The loss function needs to know the differential equation.</span>
        loss = loss_fn<span style="color: #3a81c3;">(</span>psi_trial_dash, psi_trial, x<span style="color: #3a81c3;">)</span>
        vldt_val = loss_fn<span style="color: #3a81c3;">(</span>psi_trial_dash, psi_trial, test_mesh<span style="color: #3a81c3;">)</span>
    <span style="color: #715ab1;">loss_w_dash</span>, <span style="color: #715ab1;">loss_u_dash</span>, <span style="color: #715ab1;">loss_v_dash</span> = t2.gradient<span style="color: #3a81c3;">(</span>loss, <span style="color: #6c3163;">[</span>psi_model.w, psi_model.u, psi_model.v<span style="color: #6c3163;">]</span><span style="color: #3a81c3;">)</span>
    psi_val_stash.append<span style="color: #3a81c3;">(</span>psi_model<span style="color: #6c3163;">(</span>plt_mesh<span style="color: #6c3163;">)</span>.numpy<span style="color: #6c3163;">()</span><span style="color: #3a81c3;">)</span>
    psi_model.w.assign_sub<span style="color: #3a81c3;">(</span>learning_rate * loss_w_dash<span style="color: #3a81c3;">)</span>
    psi_model.u.assign_sub<span style="color: #3a81c3;">(</span>learning_rate * loss_u_dash<span style="color: #3a81c3;">)</span>
    psi_model.v.assign_sub<span style="color: #3a81c3;">(</span>learning_rate * loss_v_dash<span style="color: #3a81c3;">)</span>
    <span style="color: #3a81c3;">if</span> iter_num % <span style="color: #4e3163;">10</span> == <span style="color: #4e3163;">0</span>:
        iter_stash.append<span style="color: #3a81c3;">(</span>iter_num<span style="color: #3a81c3;">)</span>
        loss_stash.append<span style="color: #3a81c3;">(</span>loss.numpy<span style="color: #6c3163;">()</span><span style="color: #3a81c3;">)</span>
        vldt_stash.append<span style="color: #3a81c3;">(</span>vldt_val.numpy<span style="color: #6c3163;">()</span><span style="color: #3a81c3;">)</span>
        <span style="color: #3a81c3;">print</span><span style="color: #3a81c3;">(</span><span style="color: #2d9574;">"iteration: {i}\tloss: {l}\tlearning rate: {r}"</span>.<span style="color: #3a81c3;">format</span><span style="color: #6c3163;">(</span>i=iter_num, l=loss.numpy<span style="color: #2d9574;">()</span>, r=learning_rate<span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>
    <span style="color: #3a81c3;">if</span> iter_num % <span style="color: #4e3163;">1000</span> == <span style="color: #4e3163;">0</span> <span style="color: #3a81c3;">and</span> iter_num &gt; <span style="color: #4e3163;">0</span>:
        learning_rate /= <span style="color: #4e3163;">1</span>.<span style="color: #4e3163;">5</span>

fig, axs = plt.subplots<span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span>, <span style="color: #4e3163;">2</span>, figsize=<span style="color: #6c3163;">(</span><span style="color: #4e3163;">10</span>, <span style="color: #4e3163;">5</span><span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">0</span><span style="color: #3a81c3;">]</span>.plot<span style="color: #3a81c3;">(</span>plt_mesh, psi_truth<span style="color: #6c3163;">(</span>plt_mesh<span style="color: #6c3163;">)</span>, label=<span style="color: #2d9574;">"True solution"</span><span style="color: #3a81c3;">)</span>
<span style="color: #3a81c3;">for</span> ix <span style="color: #3a81c3;">in</span> <span style="color: #3a81c3;">range</span><span style="color: #3a81c3;">(</span><span style="color: #4e3163;">1</span>, num_iters, num_iters // <span style="color: #4e3163;">3</span><span style="color: #3a81c3;">)</span>:
    axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">0</span><span style="color: #3a81c3;">]</span>.plot<span style="color: #3a81c3;">(</span>plt_mesh, psi_val_stash<span style="color: #6c3163;">[</span>ix<span style="color: #6c3163;">]</span>, label=<span style="color: #2d9574;">"Iteration {n}"</span>.<span style="color: #3a81c3;">format</span><span style="color: #6c3163;">(</span>n=ix+<span style="color: #4e3163;">1</span><span style="color: #6c3163;">)</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">0</span><span style="color: #3a81c3;">]</span>.scatter<span style="color: #3a81c3;">(</span>train_mesh, psi_truth<span style="color: #6c3163;">(</span>train_mesh<span style="color: #6c3163;">)</span>, label=<span style="color: #2d9574;">"Training data"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">0</span><span style="color: #3a81c3;">]</span>.legend<span style="color: #3a81c3;">(</span>loc=<span style="color: #2d9574;">"upper left"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">0</span><span style="color: #3a81c3;">]</span>.set_title<span style="color: #3a81c3;">(</span><span style="color: #2d9574;">"Approximate solutions of ODE"</span><span style="color: #3a81c3;">)</span>

axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">]</span>.plot<span style="color: #3a81c3;">(</span>iter_stash, loss_stash, label=<span style="color: #2d9574;">"Training"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">]</span>.plot<span style="color: #3a81c3;">(</span>iter_stash, vldt_stash, label=<span style="color: #2d9574;">"Validation"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">]</span>.set_title<span style="color: #3a81c3;">(</span><span style="color: #2d9574;">"Loss functions"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">]</span>.set_xlabel<span style="color: #3a81c3;">(</span><span style="color: #2d9574;">"Iteration"</span><span style="color: #3a81c3;">)</span>
axs<span style="color: #3a81c3;">[</span><span style="color: #4e3163;">1</span><span style="color: #3a81c3;">]</span>.legend<span style="color: #3a81c3;">(</span>loc=<span style="color: #2d9574;">"upper right"</span><span style="color: #3a81c3;">)</span>
<span style="color: #2aa1ae; background-color: #ecf3ec;"># </span><span style="color: #2aa1ae; background-color: #ecf3ec;">fig.show()</span>
fig.savefig<span style="color: #3a81c3;">(</span><span style="color: #2d9574;">"out/lagaris-1998.png"</span>, dpi=<span style="color: #4e3163;">300</span><span style="color: #3a81c3;">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org8e42416" class="outline-3">
<h3 id="org8e42416">Initialisation</h3>
<div class="outline-text-3" id="text-org8e42416">
<ul class="org-ul">
<li>When initialising the weights of a neural network, keep in mind that we want
the linear map of the input to take a value that has values roughly compatible
with a standard Gaussian. As a result, if you have \(n\) (standardised) input
variables, you might choose to initialise the weights from \(N(0,1\sqrt{n})\).</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alexander E. Zarebski</p>
<p class="date">Created: 2022-07-30 Sat 14:32</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
