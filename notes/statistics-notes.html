<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-02-08 Mon 16:29 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>aez-notes</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Alex Zarebski" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../css/stylesheet.css" />
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">aez-notes</h1>
<p>
<a href="../index.html">Home</a>
</p>

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1221e67">Statistics notes</a>
<ul>
<li><a href="#orgfb1c050">Guassian approximation to posterior</a></li>
<li><a href="#orgaa589b2">Is it statistically significant</a></li>
<li><a href="#org541eb15">Empirical Distribution Function</a></li>
<li><a href="#orgbd3a2f3">The bootstrap</a></li>
<li><a href="#org89829e6">MCMC parameter transformation</a></li>
<li><a href="#org8c82ce1">Linear regression</a></li>
<li><a href="#org53aaf94">Profile likelihood function</a></li>
<li><a href="#org56a12c3">Contingency tables</a></li>
<li><a href="#org96ac582">Linear Mixed-effects Models</a></li>
<li><a href="#org13017a8">Moment closure for CTMC processes</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org1221e67" class="outline-2">
<h2 id="org1221e67">Statistics notes</h2>
<div class="outline-text-2" id="text-org1221e67">
</div>
<div id="outline-container-orgfb1c050" class="outline-3">
<h3 id="orgfb1c050">Guassian approximation to posterior</h3>
<div class="outline-text-3" id="text-orgfb1c050">
<p>
Consider a posterior distribution where the mode is at \(\theta^{*}\) and the Hessian matrix at this point is \(h''(\theta^{*})\).
If we consider a multivariate Gaussian density, we can see that it will have its mode at the mean, \(\mu\), and, assuming a covariance matrix, \(\Sigma\), the density will have a Hessian of \(-\Sigma^{-1}\) at the mode.
This motivates the Guassian approximation at the mode of the posterior.
</p>
</div>
</div>

<div id="outline-container-orgaa589b2" class="outline-3">
<h3 id="orgaa589b2">Is it statistically significant</h3>
<div class="outline-text-3" id="text-orgaa589b2">
<p>
Here is a table of values for testing whether the a binomial sample differs from fair trials at a significance level of 0.05.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Trials</th>
<th scope="col" class="org-right">Lower</th>
<th scope="col" class="org-right">Upper</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">20</td>
<td class="org-right">6</td>
<td class="org-right">14</td>
</tr>

<tr>
<td class="org-right">40</td>
<td class="org-right">14</td>
<td class="org-right">26</td>
</tr>

<tr>
<td class="org-right">60</td>
<td class="org-right">22</td>
<td class="org-right">38</td>
</tr>

<tr>
<td class="org-right">80</td>
<td class="org-right">31</td>
<td class="org-right">49</td>
</tr>

<tr>
<td class="org-right">100</td>
<td class="org-right">40</td>
<td class="org-right">60</td>
</tr>

<tr>
<td class="org-right">200</td>
<td class="org-right">86</td>
<td class="org-right">114</td>
</tr>

<tr>
<td class="org-right">400</td>
<td class="org-right">180</td>
<td class="org-right">220</td>
</tr>

<tr>
<td class="org-right">600</td>
<td class="org-right">276</td>
<td class="org-right">324</td>
</tr>

<tr>
<td class="org-right">800</td>
<td class="org-right">372</td>
<td class="org-right">428</td>
</tr>

<tr>
<td class="org-right">1000</td>
<td class="org-right">469</td>
<td class="org-right">531</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org541eb15" class="outline-3">
<h3 id="org541eb15">Empirical Distribution Function</h3>
<div class="outline-text-3" id="text-org541eb15">
<p>
Let \(X_1,\dots,X_n\sim F\) IID on \(\mathbb{R}\), then the empirical
distribution function (EDF) is
</p>

<p>
\[
\hat{F}_n(x) = \frac{1}{n}\sum I(X_i \leq x)
\]
</p>

<p>
where \(I\) is the indicator function. Linearity shows 
</p>

<p>
\[
\mathbb{E}\hat{F}_n(x) = F(x)
\]
</p>

<p>
and basic properties of variance and the Bernoulli distribution shows
</p>

<p>
\[
\mathbb{V}\hat{F}_n(x) = \frac{1}{n} F(x) (1 - F(x)).
\]
</p>

<p>
The EDF converges to the CDF in probability (which can be observed from Markov's
inequality.) The DKW inequality bounds this convergence which allows for the
construction of confidence bounds on the EDF. A functional \(T : F \mapsto
\theta\) is called a statistical functional. The <i>plug-in estimator</i> of
\(\theta\) is simply \(T(\hat{F}_n)\).
</p>
</div>
</div>
<div id="outline-container-orgbd3a2f3" class="outline-3">
<h3 id="orgbd3a2f3">The bootstrap</h3>
<div class="outline-text-3" id="text-orgbd3a2f3">
<p>
The bootstrap is a simulation based method to estimate standard errors and
confidence intervals. Consider a statistic \(T\) which is a function of a sample
of \(X_i\sim F\); we want to compute \(\mathbb{V}_F(T)\). The subscript \(F\) is
to indicate that this is with respect to the distribution \(F\). Let \(\hat{F}\)
be the empirical distribution (EDF) of the \(X_i\). The bootstrap uses
\(\mathbb{V}_F(T)\) to approximate \(\mathbb{V}_{\hat{F}}(T)\) which is then
itself estimated via simulation.
</p>

<p>
The simulation typically is just sampling from the EDF with replacement, and we
can always run more simulation to get an arbitrarily good approximation of
\(\mathbb{V}_{\hat{F}}(T)\). The real source of error is how well \(\hat{F}\)
approximate \(F\).
</p>

<p>
In the <i>parametric</i> bootstrap, rather than sampling from the EDF of the data, a
sample is generated from the parametric distribution parameterised by the
estimated parameter value.
</p>
</div>
</div>
<div id="outline-container-org89829e6" class="outline-3">
<h3 id="org89829e6">MCMC parameter transformation</h3>
<div class="outline-text-3" id="text-org89829e6">
<p>
You want to sample \(x\) which has density \(f\) but the MCMC samples are
butting up against the edge of the support. For example, we have \(X\sim
\text{Lognormal}(0,1)\). 
</p>

<div class="org-src-container">
<pre class="src src-R"><span style="color: #4f97d7;">library</span><span style="color: #4f97d7;">(</span>mcmc<span style="color: #4f97d7;">)</span>
<span style="color: #4f97d7;">library</span><span style="color: #4f97d7;">(</span>coda<span style="color: #4f97d7;">)</span>
set.seed<span style="color: #4f97d7;">(</span><span style="color: #a45bad;">1</span><span style="color: #4f97d7;">)</span>

<span style="color: #bc6ec5; font-weight: bold;">mcmc_samples</span> <span style="color: #ce537a; font-weight: bold;">&lt;-</span> <span style="color: #4f97d7; font-weight: bold;">function</span><span style="color: #4f97d7;">(</span>obj<span style="color: #4f97d7;">)</span> <span style="color: #4f97d7;">{</span>
    as.mcmc<span style="color: #bc6ec5;">(</span>metrop<span style="color: #2d9574;">(</span>obj, <span style="color: #a45bad;">1</span>, <span style="color: #a45bad;">10000</span><span style="color: #2d9574;">)</span>$batch<span style="color: #bc6ec5;">)</span>
<span style="color: #4f97d7;">}</span>

<span style="color: #bc6ec5; font-weight: bold;">posterior</span> <span style="color: #ce537a; font-weight: bold;">&lt;-</span> <span style="color: #4f97d7; font-weight: bold;">function</span><span style="color: #4f97d7;">(</span>x<span style="color: #4f97d7;">)</span> <span style="color: #4f97d7;">{</span>
    dlnorm<span style="color: #bc6ec5;">(</span>x = x, meanlog = <span style="color: #a45bad;">0</span>, sdlog = <span style="color: #a45bad;">1</span>, log = <span style="color: #a45bad;">TRUE</span><span style="color: #bc6ec5;">)</span>
<span style="color: #4f97d7;">}</span>

x_samples <span style="color: #ce537a; font-weight: bold;">&lt;-</span> mcmc_samples<span style="color: #4f97d7;">(</span>posterior<span style="color: #4f97d7;">)</span>
</pre>
</div>

<p>
But the sampler keeps jumping into the negative numbers which is a problem. So
you might consider \(Y = log(X)\) as this takes values on the whole real line.
We need to derive the posterior distribution for this, \(g\). Note
</p>

<p>
\[
\left|g(y)dy\right| = \left|f(x)dx\right|,
\]
</p>

<p>
put another way,
</p>

<p>
\[
g(y) = f(x(y)) \left|\frac{dx(y)}{dy}\right|.
\]
</p>

<p>
So, if we let \(x = \exp(y)\), then the Jacobian is just \(\exp(y)\), and when we
take the logarithm to get the log-posterior this becomes just \(y\).
</p>

<div class="org-src-container">
<pre class="src src-R"><span style="color: #bc6ec5; font-weight: bold;">y_posterior</span> <span style="color: #ce537a; font-weight: bold;">&lt;-</span> <span style="color: #4f97d7; font-weight: bold;">function</span><span style="color: #4f97d7;">(</span>y<span style="color: #4f97d7;">)</span> <span style="color: #4f97d7;">{</span>
    posterior<span style="color: #bc6ec5;">(</span>exp<span style="color: #2d9574;">(</span>y<span style="color: #2d9574;">)</span><span style="color: #bc6ec5;">)</span> + y
<span style="color: #4f97d7;">}</span>

y_samples <span style="color: #ce537a; font-weight: bold;">&lt;-</span> mcmc_samples<span style="color: #4f97d7;">(</span>y_posterior<span style="color: #4f97d7;">)</span>
</pre>
</div>

<p>
Then a Q-Q plot of the logarithm of the \(X\) samples and the \(Y\) samples
suggests we have gotten this correct. The tails are slightly different because
the second sampler has been able to explore smaller and larger values more
efficiently.
</p>


<div id="orgba96f32" class="figure">
<p><img src="../images/mcmc-transform-demo.png" alt="mcmc-transform-demo.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org8c82ce1" class="outline-3">
<h3 id="org8c82ce1">Linear regression</h3>
<div class="outline-text-3" id="text-org8c82ce1">
</div>
<div id="outline-container-org5dd33f5" class="outline-4">
<h4 id="org5dd33f5">Linear regression with <code>lm</code></h4>
<div class="outline-text-4" id="text-org5dd33f5">
<p>
The formulas used to specify a model in R use Wilkinson-Rogers notation.
Consider \(y = \alpha + \beta x + \epsilon\). Often estimators will assume that
the \(\epsilon\) are IID normal random variables. To test whether the
\(\epsilon\) are homoscedastic one might use the <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test">Breusch-Pagan test</a>. This is
available in R via <code>lmtest::bptest</code>. To test whether the errors follow a normal
distribution, a sensible first pass would be to generate a QQ-plot. But if a
formal method is required there is the <code>stats::shapiro.test</code> function for
normality.
</p>

<p>
Assuming that for the most part the assumptions appear to be valid, we might
dive deeper into the data. The <i>leverage</i> of a datum can be thought of as the
potential to influence parameters and can be calculated with <code>stats::hatvalues</code>.
However, high leverage is not necessarily a bad thing unless it is also an
outlier. One way to measure how plausible a measurement is to have arisen from
the model is by considering its standardised residual, <code>rstandard</code>. 
</p>

<p>
Combining leverage and residual into a single measure, is the goal of the Cook's
distance which is one of the summaries produced by <code>plot.lm</code>. A rule of thumb is
that you want the Cook's distance to be not greater than \(4 N^{-1}\) for a
dataset of size \(N\).
</p>
</div>
</div>
</div>

<div id="outline-container-org53aaf94" class="outline-3">
<h3 id="org53aaf94">Profile likelihood function</h3>
<div class="outline-text-3" id="text-org53aaf94">
<p>
The profile likelihood is a lower dimensional version of the likelihood
function. Consider a likelihood function \(\mathcal{L}(\theta)\) where \(\theta
= (\psi,\lambda)\) where \(\lambda\) are nuisance parameters. The <i>profile
likelihood</i> is
</p>

<p>
\[
\mathcal{L}_{\text{profile}}(\psi) := \max_{\lambda} \mathcal{L}(\psi,\lambda).
\]
</p>
</div>
</div>

<div id="outline-container-org56a12c3" class="outline-3">
<h3 id="org56a12c3">Contingency tables</h3>
<div class="outline-text-3" id="text-org56a12c3">
</div>
<div id="outline-container-org44e1812" class="outline-4">
<h4 id="org44e1812">An example in R</h4>
<div class="outline-text-4" id="text-org44e1812">
<p>
A contingency table counts the number of times that a particular combination of
catagorical variables occur. For example, we can simulate a data set of
catagorical variables as follows
</p>

<div class="org-src-container">
<pre class="src src-R">set.seed<span style="color: #4f97d7;">(</span><span style="color: #a45bad;">1</span><span style="color: #4f97d7;">)</span>
x <span style="color: #ce537a; font-weight: bold;">&lt;-</span> sample<span style="color: #4f97d7;">(</span><span style="color: #a45bad;">1</span>:<span style="color: #a45bad;">3</span>, <span style="color: #a45bad;">1000</span>, replace = <span style="color: #a45bad;">TRUE</span><span style="color: #4f97d7;">)</span>
y <span style="color: #ce537a; font-weight: bold;">&lt;-</span> sample<span style="color: #4f97d7;">(</span>letters<span style="color: #bc6ec5;">[</span><span style="color: #a45bad;">1</span>:<span style="color: #a45bad;">3</span><span style="color: #bc6ec5;">]</span>, <span style="color: #a45bad;">1000</span>, replace = <span style="color: #a45bad;">TRUE</span><span style="color: #4f97d7;">)</span>
df <span style="color: #ce537a; font-weight: bold;">&lt;-</span> data.frame<span style="color: #4f97d7;">(</span>x, y<span style="color: #4f97d7;">)</span>
</pre>
</div>

<p>
Then we can create a contingency table from this with the <code>xtabs</code> function.
</p>

<div class="org-src-container">
<pre class="src src-R">tab <span style="color: #ce537a; font-weight: bold;">&lt;-</span> xtabs<span style="color: #4f97d7;">(</span>~ x + y, data = df<span style="color: #4f97d7;">)</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-R">&gt; print<span style="color: #4f97d7;">(</span>tab<span style="color: #4f97d7;">)</span>
   y
x     a   b   c
  <span style="color: #a45bad;">1</span> <span style="color: #a45bad;">131</span> <span style="color: #a45bad;">109</span> <span style="color: #a45bad;">111</span>
  <span style="color: #a45bad;">2</span> <span style="color: #a45bad;">100</span> <span style="color: #a45bad;">122</span> <span style="color: #a45bad;">117</span>
  <span style="color: #a45bad;">3</span> <span style="color: #a45bad;">100</span>  <span style="color: #a45bad;">99</span> <span style="color: #a45bad;">111</span>
</pre>
</div>

<p>
The null hypothesis test that we are interested in is that there is no
association between the catagorical variables <code>x</code> and <code>y</code>. If each variable was
binary we could use Fisher's exact test, but since there are more, and there are
\(\geq 10\) observations in each catagory a $&chi;<sup>2</sup>$-test is acceptable.
</p>

<div class="org-src-container">
<pre class="src src-R">&gt; print<span style="color: #4f97d7;">(</span>chisq.test<span style="color: #bc6ec5;">(</span>tab<span style="color: #bc6ec5;">)</span><span style="color: #4f97d7;">)</span>

  Pearson<span style="color: #2d9574;">'s Chi-squared test</span>

<span style="color: #2d9574;">data:  tab</span>
<span style="color: #2d9574;">X-squared = 5.6178, df = 4, p-value = 0.2296</span>

</pre>
</div>

<p>
Since the variables were simulated independently the $p$-value is, not
surprisingly, large enough that it would not be considered significant.
</p>
</div>
</div>

<div id="outline-container-org1667dcd" class="outline-4">
<h4 id="org1667dcd">What actually happens</h4>
<div class="outline-text-4" id="text-org1667dcd">
<p>
Let \(f_{ij}\) be the value in the \(ij\)-th cell of the contingency table and
\(e_{ij}\) the expected value assuming that the observations are distributed such
that the catagorical variables are independent. Consider the following
statistic:
</p>

<p>
\[
\sum_{(i,j)} \frac{ (f_{ij} - e_{ij})^2 }{ e_{ij} }
\]
</p>

<p>
This statistic has a \(\chi^2\)-distribution with \((I-1)(J-1)\) degrees of
freedom where \(I\) and \(J\) are the number of distinct values each variable
takes.
</p>
</div>
</div>
</div>

<div id="outline-container-org96ac582" class="outline-3">
<h3 id="org96ac582">Linear Mixed-effects Models</h3>
<div class="outline-text-3" id="text-org96ac582">
<p>
The Laird-Ware form of a linear mixed effect model (LMM) for the \(j\)th
observation in the \(i\)th group of measurements is as follows:
</p>

<p>
\[
Y_{ij} = \beta_1 + \sum_k \beta_k X_{kij} + \sum_{k} \delta_{ki} Z_{kij} + \epsilon_{ij}.
\]
</p>

<ul class="org-ul">
<li>the \(\beta_k\) are the <i>fixed effect</i> coefficients and the \(X_{kij}\) the
fixed effect covariates,</li>
<li>the \(\delta_k\) are the <i>random effect</i> coefficients and the \(Z_{kij}\) the
random effect covariates, it is important to note that while the \(beta_k\)
are treated as parameters to be estimated, the \(\delta_k\) are treated as
random variables and it is their distribution that is estimated.</li>
<li>the \(\epsilon_{ij}\) is a random variable.</li>
<li>the distribution of the random effect coefficients is a (zero-mean)
multivariate normal distribution parameterised by \(\psi\) and the random
noise \(\epsilon\) comes from a zero-mean normal distribution with variance
parameterised by \(\sigma^2\).</li>
</ul>
</div>

<div id="outline-container-org00bba30" class="outline-4">
<h4 id="org00bba30">Model derivation</h4>
<div class="outline-text-4" id="text-org00bba30">
<p>
One way to go about deriving a LMM for a particular data set is to consider a
model at the individual level and then assume some random structure on the
parameters which varies at the group level. Expanding this out will lead to a
model in the Laird-Ware form. The random variables in the model at the group
level creates the random effects terms in the Laird-Ware form where the constant
parts of the parameters form the fixed effects.
</p>
</div>
</div>

<div id="outline-container-org91c3566" class="outline-4">
<h4 id="org91c3566">Model notation</h4>
<div class="outline-text-4" id="text-org91c3566">
<p>
The <code>lme4</code> package in R introduces some syntax for describing these models. 
</p>

<ul class="org-ul">
<li><code>(expr | factor)</code> is used to indicate that the expression <code>expr</code> represents
random effects and that these values should be common across <code>factor</code>. By
default, this assumes that there are correlations between the random effects.</li>
<li><code>(expr || factor)</code> is another way to specify that the <code>expr</code> are random
effects, but assumes that they are uncorrelated.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org13017a8" class="outline-3">
<h3 id="org13017a8">Moment closure for CTMC processes</h3>
<div class="outline-text-3" id="text-org13017a8">
<p>
Consider a random process, \(X_t\) which is a CTMC on \(\mathbb{N}_0\) where the
only possible transitions from \(n\) are to \(n\pm 1\), potentially with an
absorbing state at zero. Supposing from state \(n\) the process moves to state
\(n+1\) at rate \(a_n\) and to state \(n-1\) at rate \(b_n\) the forward
equations for the distribution are
</p>

<p>
\[
\frac{d}{dt} p_n(t) = p_{n-1}(t)a_{n-1} - p_{n}(t)a_n + p_{n+1}(t)b_{n+1} - p_{n}(t)b_{n}.
\]
</p>

<p>
We multiple both sides of the equation through by \(n\) and sum in \(n\) over
\(\mathbb{N}_0\) to get everything in terms of expected values of functions of
\(X_t\). 
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t] = \mathbb{E}[((X_t + 1) - X_t) a_{X_t}] + \mathbb{E}[((X_t - 1) - X_t) b_{X_t}] 
\]
</p>

<p>
The "trick" is to note that you can write the first and third sums as \(\sum
(n+1) p_{n}(t) a_{n}\) and \(\sum (n-1) p_n b_n\). Similarly, for higher order
moments this generalises as
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \mathbb{E}[((X_t + 1)^k - X_t^k) a_{X_t}] + \mathbb{E}[((X_t - 1)^k - X_t^k) b_{X_t}].
\]
</p>

<p>
Recall the binomial formula, 
</p>

<p>
\[
(x+y)^k = \sum_{i = 0}^{k} \binom{k}{i} x^i y^{k-i}.
\]
</p>

<p>
This can then be used to derive the differential equations for the moments.
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \sum_{i = 0}^{k-1} \binom{k}{i} \left(  \mathbb{E}[ X_t^i a_{X_t} ]  + (-1)^{k-i} \mathbb{E}[ X_t^i b_{X_t} ] \right)
\]
</p>

<p>
Note that the sums are from \(0\) to \(k-1\) but the binomial coefficients use
the full \(k\). Given this expression, any polynomial \(a_n\) and \(b_n\) are
candidates for the application of a moment closure, which essentially just
entails either truncating the cumulants, or assuming a distributional form which
expresses higher moments in terms of lower ones.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alex Zarebski</p>
<p class="date">Created: 2021-02-08 Mon 16:29</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
