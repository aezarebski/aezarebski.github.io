<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-11 Mon 11:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>statistics-aez-notes</title>
<meta name="author" content="Alex Zarebski" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
<link id="stylesheet" rel="stylesheet" type="text/css" href="../css/stylesheet-dark.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">statistics-aez-notes</h1>
<p>
<a href="../index.html">Home</a>
</p>

<button type="button" onclick="toggleStyle()">Light/Dark</button>
<script src="./main.js"></script>

<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org84ab68a">Statistics notes</a>
<ul>
<li><a href="#org31b116c">Fisher information</a></li>
<li><a href="#org838cce9">Guassian approximation to posterior</a></li>
<li><a href="#orgc4082d8">Is it statistically significant</a></li>
<li><a href="#org95c788d">Empirical Distribution Function</a></li>
<li><a href="#org855ffd7">The bootstrap</a></li>
<li><a href="#orgac9e9e8">MCMC parameter transformation</a></li>
<li><a href="#orgc99ccaf">Linear regression</a></li>
<li><a href="#org15d45de">Profile likelihood function</a></li>
<li><a href="#orgcb469f8"><span class="todo TODO">TODO</span> Student's <i>t</i>-test</a></li>
<li><a href="#org8f15f1a"><span class="todo TODO">TODO</span> Wilcoxon signed-rank test</a></li>
<li><a href="#org86fc8a4"><span class="todo TODO">TODO</span> Mann–Whitney <i>U</i> test</a></li>
<li><a href="#orgca92d5a"><span class="todo TODO">TODO</span> Kruskal–Wallis <i>H</i> test</a></li>
<li><a href="#orgd9215a4">Contingency tables</a></li>
<li><a href="#org743d232">Linear Mixed-effects Models</a></li>
<li><a href="#org53e90d2">Moment closure for CTMC processes</a></li>
<li><a href="#org1d025ba">The Law of the unconscious statistician</a></li>
<li><a href="#org94c2075"><span class="todo TODO">TODO</span> Bland-Altman plot</a></li>
<li><a href="#org0a86a90">Further reading</a></li>
</ul>
</li>
<li><a href="#orgc9bd398">Machine learning notes</a>
<ul>
<li><a href="#orgd0fd016">Using neural networks to solve differential equations</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org84ab68a" class="outline-2">
<h2 id="org84ab68a">Statistics notes</h2>
<div class="outline-text-2" id="text-org84ab68a">
</div>
<div id="outline-container-org31b116c" class="outline-3">
<h3 id="org31b116c">Fisher information</h3>
<div class="outline-text-3" id="text-org31b116c">
<p>
For a random variable, \(X\) with distrtibution \(f(X;\theta)\) the <i>score
function</i> is
</p>

<p>
\[
s(X; \theta) = \frac{\partial}{\partial\theta} \log f(X:\theta).
\]
</p>

<p>
Under some mild assumptions, the mean value of \(s(X; \theta)\) (when viewed as
a function of \(X\)) is zero.
</p>

<p>
The <i>Fisher information</i> is defined as the variance of the score and denoted \(\mathcal{I}(\theta)\):
</p>

<p>
\[
\mathcal{I}(\theta) = \mathbb{E}\left[ \left(\frac{\partial}{\partial\theta} \log f(X:\theta) \right)^{2} \right].
\]
</p>

<p>
It can be shown that the Fisher information is equal to
</p>

<p>
\[
-\mathbb{E}\left[ \frac{\partial^{2}}{\partial\theta^{2}} \log f(X:\theta) \right]
\]
</p>

<p>
which can be more convenient to work with.
</p>
</div>
</div>

<div id="outline-container-org838cce9" class="outline-3">
<h3 id="org838cce9">Guassian approximation to posterior</h3>
<div class="outline-text-3" id="text-org838cce9">
<p>
Consider a posterior distribution where the mode is at \(\theta^{*}\) and the
Hessian matrix at this point is \(h''(\theta^{*})\). If we consider a
multivariate Gaussian density, we can see that it will have its mode at the
mean, \(\mu\), and, assuming a covariance matrix, \(\Sigma\), the density will
have a Hessian of \(-\Sigma^{-1}\) at the mode. This motivates the Guassian
approximation at the mode of the posterior. Also see <a href="#org31b116c">Fisher information</a>.
</p>
</div>
</div>

<div id="outline-container-orgc4082d8" class="outline-3">
<h3 id="orgc4082d8">Is it statistically significant</h3>
<div class="outline-text-3" id="text-orgc4082d8">
<p>
Here is a table of values for testing whether the a binomial sample differs from
fair trials at a significance level of 0.05.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Trials</th>
<th scope="col" class="org-right">Lower</th>
<th scope="col" class="org-right">Upper</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">20</td>
<td class="org-right">6</td>
<td class="org-right">14</td>
</tr>

<tr>
<td class="org-right">40</td>
<td class="org-right">14</td>
<td class="org-right">26</td>
</tr>

<tr>
<td class="org-right">60</td>
<td class="org-right">22</td>
<td class="org-right">38</td>
</tr>

<tr>
<td class="org-right">80</td>
<td class="org-right">31</td>
<td class="org-right">49</td>
</tr>

<tr>
<td class="org-right">100</td>
<td class="org-right">40</td>
<td class="org-right">60</td>
</tr>

<tr>
<td class="org-right">200</td>
<td class="org-right">86</td>
<td class="org-right">114</td>
</tr>

<tr>
<td class="org-right">400</td>
<td class="org-right">180</td>
<td class="org-right">220</td>
</tr>

<tr>
<td class="org-right">600</td>
<td class="org-right">276</td>
<td class="org-right">324</td>
</tr>

<tr>
<td class="org-right">800</td>
<td class="org-right">372</td>
<td class="org-right">428</td>
</tr>

<tr>
<td class="org-right">1000</td>
<td class="org-right">469</td>
<td class="org-right">531</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org95c788d" class="outline-3">
<h3 id="org95c788d">Empirical Distribution Function</h3>
<div class="outline-text-3" id="text-org95c788d">
<p>
Let \(X_1,\dots,X_n\sim F\) IID on \(\mathbb{R}\), then the empirical
distribution function (EDF) is
</p>

<p>
\[
\hat{F}_n(x) = \frac{1}{n}\sum I(X_i \leq x)
\]
</p>

<p>
where \(I\) is the indicator function. Linearity shows
</p>

<p>
\[
\mathbb{E}\hat{F}_n(x) = F(x)
\]
</p>

<p>
and basic properties of variance and the Bernoulli distribution shows
</p>

<p>
\[
\mathbb{V}\hat{F}_n(x) = \frac{1}{n} F(x) (1 - F(x)).
\]
</p>

<p>
The EDF converges to the CDF in probability (which can be observed from Markov's
inequality.) The DKW inequality bounds this convergence which allows for the
construction of confidence bounds on the EDF. A functional \(T : F \mapsto
\theta\) is called a statistical functional. The <i>plug-in estimator</i> of
\(\theta\) is simply \(T(\hat{F}_n)\).
</p>
</div>
</div>

<div id="outline-container-org855ffd7" class="outline-3">
<h3 id="org855ffd7">The bootstrap</h3>
<div class="outline-text-3" id="text-org855ffd7">
<p>
The bootstrap is a simulation based method to estimate standard errors and
confidence intervals. Consider a statistic \(T\) which is a function of a sample
of \(X_i\sim F\); we want to compute \(\mathbb{V}_F(T)\). The subscript \(F\) is
to indicate that this is with respect to the distribution \(F\). Let \(\hat{F}\)
be the empirical distribution (EDF) of the \(X_i\). The bootstrap uses
\(\mathbb{V}_F(T)\) to approximate \(\mathbb{V}_{\hat{F}}(T)\) which is then
itself estimated via simulation.
</p>

<p>
The simulation typically is just sampling from the EDF with replacement, and we
can always run more simulation to get an arbitrarily good approximation of
\(\mathbb{V}_{\hat{F}}(T)\). The real source of error is how well \(\hat{F}\)
approximate \(F\).
</p>

<p>
In the <i>parametric</i> bootstrap, rather than sampling from the EDF of the data, a
sample is generated from the parametric distribution parameterised by the
estimated parameter value.
</p>
</div>
</div>

<div id="outline-container-orgac9e9e8" class="outline-3">
<h3 id="orgac9e9e8">MCMC parameter transformation</h3>
<div class="outline-text-3" id="text-orgac9e9e8">
<p>
You want to sample \(x\) which has density \(f\) but the MCMC samples are
butting up against the edge of the support. For example, we have \(X\sim
\text{Lognormal}(0,1)\).
</p>

<div class="org-src-container">
<pre class="src src-R">library(mcmc)
library(coda)
set.seed(1)

mcmc_samples &lt;- function(obj) {
    as.mcmc(metrop(obj, 1, 10000)$batch)
}

posterior &lt;- function(x) {
    dlnorm(x = x, meanlog = 0, sdlog = 1, log = TRUE)
}

x_samples &lt;- mcmc_samples(posterior)
</pre>
</div>

<p>
But the sampler keeps jumping into the negative numbers which is a problem. So
you might consider \(Y = log(X)\) as this takes values on the whole real line.
We need to derive the posterior distribution for this, \(g\). Note
</p>

<p>
\[
\left|g(y)dy\right| = \left|f(x)dx\right|,
\]
</p>

<p>
put another way,
</p>

<p>
\[
g(y) = f(x(y)) \left|\frac{dx(y)}{dy}\right|.
\]
</p>

<p>
So, if we let \(x = \exp(y)\), then the Jacobian is just \(\exp(y)\), and when we
take the logarithm to get the log-posterior this becomes just \(y\).
</p>

<div class="org-src-container">
<pre class="src src-R">y_posterior &lt;- function(y) {
    posterior(exp(y)) + y
}

y_samples &lt;- mcmc_samples(y_posterior)
</pre>
</div>

<p>
Then a Q-Q plot of the logarithm of the \(X\) samples and the \(Y\) samples
suggests we have gotten this correct. The tails are slightly different because
the second sampler has been able to explore smaller and larger values more
efficiently.
</p>


<div id="org2e116b3" class="figure">
<p><img src="../images/mcmc-transform-demo.png" alt="mcmc-transform-demo.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgc99ccaf" class="outline-3">
<h3 id="orgc99ccaf">Linear regression</h3>
<div class="outline-text-3" id="text-orgc99ccaf">
</div>
<div id="outline-container-orgdd6a9a4" class="outline-4">
<h4 id="orgdd6a9a4">Linear regression with <code>lm</code></h4>
<div class="outline-text-4" id="text-orgdd6a9a4">
<p>
The formulas used to specify a model in R use Wilkinson-Rogers notation.
Consider \(y = \alpha + \beta x + \epsilon\). Often estimators will assume that
the \(\epsilon\) are IID normal random variables. To test whether the
\(\epsilon\) are homoscedastic one might use the <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test">Breusch-Pagan test</a>. This is
available in R via <code>lmtest::bptest</code>. To test whether the errors follow a normal
distribution, a sensible first pass would be to generate a QQ-plot. But if a
formal method is required there is the <code>stats::shapiro.test</code> function for
normality.
</p>

<p>
Assuming that for the most part the assumptions appear to be valid, we might
dive deeper into the data. The <i>leverage</i> of a datum can be thought of as the
potential to influence parameters and can be calculated with <code>stats::hatvalues</code>.
However, high leverage is not necessarily a bad thing unless it is also an
outlier. One way to measure how plausible a measurement is to have arisen from
the model is by considering its standardised residual, <code>rstandard</code>.
</p>

<p>
Combining leverage and residual into a single measure, is the goal of the Cook's
distance which is one of the summaries produced by <code>plot.lm</code>. A rule of thumb is
that you want the Cook's distance to be not greater than \(4 N^{-1}\) for a
dataset of size \(N\).
</p>
</div>
</div>
</div>

<div id="outline-container-org15d45de" class="outline-3">
<h3 id="org15d45de">Profile likelihood function</h3>
<div class="outline-text-3" id="text-org15d45de">
<p>
The profile likelihood is a lower dimensional version of the likelihood
function. Consider a likelihood function \(\mathcal{L}(\theta)\) where \(\theta
= (\psi,\lambda)\) where \(\lambda\) are nuisance parameters. The <i>profile
likelihood</i> is
</p>

<p>
\[
\mathcal{L}_{\text{profile}}(\psi) := \max_{\lambda} \mathcal{L}(\psi,\lambda).
\]
</p>
</div>
</div>

<div id="outline-container-orgcb469f8" class="outline-3">
<h3 id="orgcb469f8"><span class="todo TODO">TODO</span> Student's <i>t</i>-test</h3>
<div class="outline-text-3" id="text-orgcb469f8">
</div>
<div id="outline-container-orge5f321b" class="outline-4">
<h4 id="orge5f321b">Null is specific mean</h4>
<div class="outline-text-4" id="text-orge5f321b">
<div class="org-src-container">
<pre class="src src-R">t.test(rnorm(100, mean=1), mu = 0)$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-org44fe4b9" class="outline-4">
<h4 id="org44fe4b9">Null is different mean (assumed equal variance)</h4>
<div class="outline-text-4" id="text-org44fe4b9">
<div class="org-src-container">
<pre class="src src-R">t.test(rnorm(100, mean = 0), rnorm(100, mean = 1), var.equal = TRUE)$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-orgb87d97f" class="outline-4">
<h4 id="orgb87d97f">Null is different mean (Welch)</h4>
<div class="outline-text-4" id="text-orgb87d97f">
<div class="org-src-container">
<pre class="src src-R">t.test(rnorm(100, mean = 0), rnorm(100, mean = 1, sd = 2), var.equal = FALSE)$p.value
</pre>
</div>
</div>
</div>

<div id="outline-container-org7fe1757" class="outline-4">
<h4 id="org7fe1757"><span class="todo TODO">TODO</span> Null is different mean but values are paired</h4>
</div>
</div>

<div id="outline-container-org8f15f1a" class="outline-3">
<h3 id="org8f15f1a"><span class="todo TODO">TODO</span> Wilcoxon signed-rank test</h3>
<div class="outline-text-3" id="text-org8f15f1a">
<p>
Roughly, it plays the role of a nonparametric paired <i>t</i>-test.
</p>
</div>
</div>

<div id="outline-container-org86fc8a4" class="outline-3">
<h3 id="org86fc8a4"><span class="todo TODO">TODO</span> Mann–Whitney <i>U</i> test</h3>
<div class="outline-text-3" id="text-org86fc8a4">
<p>
Roughly, it plays the role of a nonparametric <i>t</i>-test.
</p>
</div>
</div>

<div id="outline-container-orgca92d5a" class="outline-3">
<h3 id="orgca92d5a"><span class="todo TODO">TODO</span> Kruskal–Wallis <i>H</i> test</h3>
<div class="outline-text-3" id="text-orgca92d5a">
<p>
Roughly, it extends the <a href="#org86fc8a4">Mann–Whitney <i>U</i> test</a> to more than two groups.
</p>
</div>
</div>

<div id="outline-container-orgd9215a4" class="outline-3">
<h3 id="orgd9215a4">Contingency tables</h3>
<div class="outline-text-3" id="text-orgd9215a4">
</div>
<div id="outline-container-org2623605" class="outline-4">
<h4 id="org2623605">An example in R</h4>
<div class="outline-text-4" id="text-org2623605">
<p>
A contingency table counts the number of times that a particular combination of
catagorical variables occur. For example, we can simulate a data set of
catagorical variables as follows
</p>

<div class="org-src-container">
<pre class="src src-R">set.seed(1)
x &lt;- sample(1:3, 1000, replace = TRUE)
y &lt;- sample(letters[1:3], 1000, replace = TRUE)
df &lt;- data.frame(x, y)
</pre>
</div>

<p>
Then we can create a contingency table from this with the <code>xtabs</code> function.
</p>

<div class="org-src-container">
<pre class="src src-R">tab &lt;- xtabs(~ x + y, data = df)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-R">&gt; print(tab)
   y
x     a   b   c
  1 131 109 111
  2 100 122 117
  3 100  99 111
</pre>
</div>

<p>
The null hypothesis test that we are interested in is that there is no
association between the catagorical variables <code>x</code> and <code>y</code>. If each variable was
binary we could use Fisher's exact test, but since there are more, and there are
\(\geq 10\) observations in each catagory a $&chi;<sup>2</sup>$-test is acceptable.
</p>

<div class="org-src-container">
<pre class="src src-R">&gt; print(chisq.test(tab))

	Pearson's Chi-squared test

data:  tab
X-squared = 5.6178, df = 4, p-value = 0.2296

</pre>
</div>

<p>
Since the variables were simulated independently the $p$-value is, not
surprisingly, large enough that it would not be considered significant.
</p>
</div>
</div>

<div id="outline-container-orgeddb45b" class="outline-4">
<h4 id="orgeddb45b">What actually happens</h4>
<div class="outline-text-4" id="text-orgeddb45b">
<p>
Let \(f_{ij}\) be the value in the \(ij\)-th cell of the contingency table and
\(e_{ij}\) the expected value assuming that the observations are distributed such
that the catagorical variables are independent. Consider the following
statistic:
</p>

<p>
\[
\sum_{(i,j)} \frac{ (f_{ij} - e_{ij})^2 }{ e_{ij} }
\]
</p>

<p>
This statistic has a \(\chi^2\)-distribution with \((I-1)(J-1)\) degrees of
freedom where \(I\) and \(J\) are the number of distinct values each variable
takes.
</p>
</div>
</div>
</div>

<div id="outline-container-org743d232" class="outline-3">
<h3 id="org743d232">Linear Mixed-effects Models</h3>
<div class="outline-text-3" id="text-org743d232">
<p>
The Laird-Ware form of a linear mixed effect model (LMM) for the \(j\)th
observation in the \(i\)th group of measurements is as follows:
</p>

<p>
\[
Y_{ij} = \beta_1 + \sum_k \beta_k X_{kij} + \sum_{k} \delta_{ki} Z_{kij} + \epsilon_{ij}.
\]
</p>

<ul class="org-ul">
<li>the \(\beta_k\) are the <i>fixed effect</i> coefficients and the \(X_{kij}\) the
fixed effect covariates,</li>
<li>the \(\delta_k\) are the <i>random effect</i> coefficients and the \(Z_{kij}\) the
random effect covariates, it is important to note that while the \(beta_k\)
are treated as parameters to be estimated, the \(\delta_k\) are treated as
random variables and it is their distribution that is estimated.</li>
<li>the \(\epsilon_{ij}\) is a random variable.</li>
<li>the distribution of the random effect coefficients is a (zero-mean)
multivariate normal distribution parameterised by \(\psi\) and the random
noise \(\epsilon\) comes from a zero-mean normal distribution with variance
parameterised by \(\sigma^2\).</li>
</ul>
</div>

<div id="outline-container-org091d5c6" class="outline-4">
<h4 id="org091d5c6">Model derivation</h4>
<div class="outline-text-4" id="text-org091d5c6">
<p>
One way to go about deriving a LMM for a particular data set is to consider a
model at the individual level and then assume some random structure on the
parameters which varies at the group level. Expanding this out will lead to a
model in the Laird-Ware form. The random variables in the model at the group
level creates the random effects terms in the Laird-Ware form where the constant
parts of the parameters form the fixed effects.
</p>
</div>
</div>

<div id="outline-container-org2c38c2f" class="outline-4">
<h4 id="org2c38c2f">Model notation</h4>
<div class="outline-text-4" id="text-org2c38c2f">
<p>
The <code>lme4</code> package in R introduces some syntax for describing these models.
</p>

<ul class="org-ul">
<li><code>(expr | factor)</code> is used to indicate that the expression <code>expr</code> represents
random effects and that these values should be common across <code>factor</code>. By
default, this assumes that there are correlations between the random effects.</li>
<li><code>(expr || factor)</code> is another way to specify that the <code>expr</code> are random
effects, but assumes that they are uncorrelated.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org53e90d2" class="outline-3">
<h3 id="org53e90d2">Moment closure for CTMC processes</h3>
<div class="outline-text-3" id="text-org53e90d2">
<p>
Consider a random process, \(X_t\) which is a CTMC on \(\mathbb{N}_0\) where the
only possible transitions from \(n\) are to \(n\pm 1\), potentially with an
absorbing state at zero. Supposing from state \(n\) the process moves to state
\(n+1\) at rate \(a_n\) and to state \(n-1\) at rate \(b_n\) the forward
equations for the distribution are
</p>

<p>
\[
\frac{d}{dt} p_n(t) = p_{n-1}(t)a_{n-1} - p_{n}(t)a_n + p_{n+1}(t)b_{n+1} - p_{n}(t)b_{n}.
\]
</p>

<p>
We multiple both sides of the equation through by \(n\) and sum in \(n\) over
\(\mathbb{N}_0\) to get everything in terms of expected values of functions of
\(X_t\).
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t] = \mathbb{E}[((X_t + 1) - X_t) a_{X_t}] + \mathbb{E}[((X_t - 1) - X_t) b_{X_t}]
\]
</p>

<p>
The "trick" is to note that you can write the first and third sums as \(\sum
(n+1) p_{n}(t) a_{n}\) and \(\sum (n-1) p_n b_n\). Similarly, for higher order
moments this generalises as
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \mathbb{E}[((X_t + 1)^k - X_t^k) a_{X_t}] + \mathbb{E}[((X_t - 1)^k - X_t^k) b_{X_t}].
\]
</p>

<p>
Recall the binomial formula,
</p>

<p>
\[
(x+y)^k = \sum_{i = 0}^{k} \binom{k}{i} x^i y^{k-i}.
\]
</p>

<p>
This can then be used to derive the differential equations for the moments.
</p>

<p>
\[
\frac{d}{dt} \mathbb{E}[X_t^k] = \sum_{i = 0}^{k-1} \binom{k}{i} \left(  \mathbb{E}[ X_t^i a_{X_t} ]  + (-1)^{k-i} \mathbb{E}[ X_t^i b_{X_t} ] \right)
\]
</p>

<p>
Note that the sums are from \(0\) to \(k-1\) but the binomial coefficients use
the full \(k\). Given this expression, any polynomial \(a_n\) and \(b_n\) are
candidates for the application of a moment closure, which essentially just
entails either truncating the cumulants, or assuming a distributional form which
expresses higher moments in terms of lower ones.
</p>
</div>
</div>

<div id="outline-container-org1d025ba" class="outline-3">
<h3 id="org1d025ba">The Law of the unconscious statistician</h3>
<div class="outline-text-3" id="text-org1d025ba">
<p>
This refers to the equation
</p>

<p>
\[
\mathbb{E}g(X) = \int g(x) f_{X}(x) dx.
\]
</p>

<p>
The name derives from the fact that this is so often treated as self-evident
rather than being viewed as a theorem.
</p>

<p>
This result is particularly useful when carrying out MCMC in a transformed
parameter space as described in the <a href="#orgac9e9e8">MCMC parameter transformation</a> notes.
</p>
</div>
</div>

<div id="outline-container-org94c2075" class="outline-3">
<h3 id="org94c2075"><span class="todo TODO">TODO</span> Bland-Altman plot</h3>
<div class="outline-text-3" id="text-org94c2075">
<p>
Given to ways to measure something, a Bland-Altman plot is a way to compare
them.
</p>
</div>
</div>

<div id="outline-container-org0a86a90" class="outline-3">
<h3 id="org0a86a90">Further reading</h3>
<div class="outline-text-3" id="text-org0a86a90">
<ul class="org-ul">
<li><a href="https://pages.cs.wisc.edu/~tdw/files/cookbook-en.pdf">Probability and Statistics Cookbook</a> from <a href="http://matthias.vallentin.net/">Matthias Vallentin</a>.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc9bd398" class="outline-2">
<h2 id="orgc9bd398">Machine learning notes</h2>
<div class="outline-text-2" id="text-orgc9bd398">
<blockquote>
<p>
The first rule of machine learning: Start without machine learning
</p>

<p>
— Eugene Yan (@eugeneyan) September 10, 2021
</p>
</blockquote>
</div>

<div id="outline-container-orgd0fd016" class="outline-3">
<h3 id="orgd0fd016">Using neural networks to solve differential equations</h3>
<div class="outline-text-3" id="text-orgd0fd016">
<p>
Lagaris <i>et al</i> (1998) describe a method to solve differential equations
numerically with neural networks. The simplest example they use is the following
differential equation:
</p>

<p>
\[
\frac{d\Psi}{dx} + \left(x + \frac{1 + 3x^{2}}{1 + x + x^{3}}\right) \Psi = x^{3} + 2x+x^{2}\frac{1 +3x^{2}}{1 + x + x^{3}}
\]
</p>

<p>
with the initial condition that \(\Psi(0) = A\) for \(0\leq x\leq 1\). The
solution to this equation is available in closed form:
</p>

<p>
\[
\Psi = \frac{e^{-x^{2} / 2}}{1 + x + x^{3}} + x^{2}.
\]
</p>

<p>
The trial solution proposed is
</p>

<p>
\[
\Psi_{t} = A + x N(x,p)
\]
</p>

<p>
where \(N\) is a neural network parameterised by \(p\). This trial solution
satisfied the initial condition for all values of \(p\) which leads to an
unconstrained optimisation problem. The loss function is
</p>

<p>
\[
\text{loss}(p) = \sum_{i} \left\{ \frac{d\Psi_{t}}{dx_{i}} + \left(x_{i} + \frac{1 + 3x_{i}^{2}}{1 + x_{i} + x_{i}^{3}}\right) \Psi_{t} - \left[ x_{i}^{3} + 2x_{i}+x_{i}^{2}\frac{1 +3x_{i}^{2}}{1 + x_{i} + x_{i}^{3}} \right] \right\}^{2}
\]
</p>

<p>
for some set of trial points \(x_{i}\). There are closed forms available for
both the loss function and its gradient which opens the way for multiple
optimisation routines to be applied.
</p>

<p>
Figure <a href="#orgb0286b3">1</a> shows the result of less than a minute of training with
gradient descent with only ten test points.
</p>


<div id="orgb0286b3" class="figure">
<p><img src="./img/lagaris-1998.png" alt="lagaris-1998.png" width="900px" />
</p>
<p><span class="figure-number">Figure 1: </span>Replication of the first example problem from Lagaris <i>et al</i> (1998)</p>
</div>

<p>
The script that generated the image in Figure <a href="#orgb0286b3">1</a> is shown below.
</p>

<div class="org-src-container">
<pre class="src src-python">import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1)

H = 10
mesh_size = 10
# We need to select some points to train the model on and some to test it on.
# It looks like you need to have this as a mutable value if you want to compute
# gradients.
train_mesh = tf.Variable(np.linspace(0, 1, mesh_size), dtype=np.float64)
test_mesh = tf.Variable(np.linspace(0.01, 0.99, mesh_size), dtype=np.float64)
# We need to select some points to plot the resulting approximation on.
plt_mesh = np.linspace(0, 2, 100)
# We need to record the solution at multiple points of the training loop to
# test that it is infact converging so something sensible.
psi_val_stash = []
iter_stash = []
loss_stash = []
vldt_stash = []

@tf.function
def psi_truth(x):
    """Solution of the differential equation."""
    return tf.exp(-0.5 * x**2) / (1 + x + x**3) + x**2


class MyModel(tf.Module):
    """This class respresents the approximation to the solution."""

    def __init__(self, psi_0, num_units, **kwargs):
	"""Construct approximate solution with a hard-coded IC."""
	super().__init__(**kwargs)
	self.IC_A = psi_0
	self.w = tf.Variable(np.random.randn(num_units))
	self.u = tf.Variable(np.random.randn(num_units))
	self.v = tf.Variable(np.random.randn(num_units))

    def __call__(self, x):
	"""Evaluate the approximation."""
	# matrix with one datum per row
	l1 = tf.math.sigmoid(tf.tensordot(x, self.w, 0) + self.u)
	# We have encoded the IC using the method from Lagaris et al (1998),
	# although there are other formulations that are used in contemporary
	# implementations. Attempting to use a 1-exp(-x) function to get the IC
	# constraint did not yield any improvement.
	l2 = self.IC_A + x * tf.tensordot(l1, self.v, 1)
	return l2


psi_model = MyModel(tf.constant(1.0, dtype=np.float64), H)

num_iters = 2000
learning_rate = 0.050
x = train_mesh


def loss_fn(psi_dash, psi, x):
    return tf.reduce_sum((psi_trial_dash + (x + (1 + 3 * x**2) / (1 + x + x**3)) * psi_trial - (x**3 + 2 * x + x**2 * (1 + 3 * x**2) / (1 + x + x**3)))**2)


for iter_num in range(num_iters):
    # The gradient evaluations need to be within to the loop for TF to
    # understand how they work.
    with tf.GradientTape() as t2:
	with tf.GradientTape() as t1:
	    psi_trial = psi_model(x)
	psi_trial_dash = t1.gradient(psi_trial, x)
	# The loss function needs to know the differential equation.
	loss = loss_fn(psi_trial_dash, psi_trial, x)
	vldt_val = loss_fn(psi_trial_dash, psi_trial, test_mesh)
    loss_w_dash, loss_u_dash, loss_v_dash = t2.gradient(loss, [psi_model.w, psi_model.u, psi_model.v])
    psi_val_stash.append(psi_model(plt_mesh).numpy())
    psi_model.w.assign_sub(learning_rate * loss_w_dash)
    psi_model.u.assign_sub(learning_rate * loss_u_dash)
    psi_model.v.assign_sub(learning_rate * loss_v_dash)
    if iter_num % 10 == 0:
	iter_stash.append(iter_num)
	loss_stash.append(loss.numpy())
	vldt_stash.append(vldt_val.numpy())
	print("iteration: {i}\tloss: {l}\tlearning rate: {r}".format(i=iter_num, l=loss.numpy(), r=learning_rate))
    if iter_num % 1000 == 0 and iter_num &gt; 0:
	learning_rate /= 1.5

fig, axs = plt.subplots(1, 2, figsize=(10, 5))
axs[0].plot(plt_mesh, psi_truth(plt_mesh), label="True solution")
for ix in range(1, num_iters, num_iters // 3):
    axs[0].plot(plt_mesh, psi_val_stash[ix], label="Iteration {n}".format(n=ix+1))
axs[0].scatter(train_mesh, psi_truth(train_mesh), label="Training data")
axs[0].legend(loc="upper left")
axs[0].set_title("Approximate solutions of ODE")

axs[1].plot(iter_stash, loss_stash, label="Training")
axs[1].plot(iter_stash, vldt_stash, label="Validation")
axs[1].set_title("Loss functions")
axs[1].set_xlabel("Iteration")
axs[1].legend(loc="upper right")
# fig.show()
fig.savefig("out/lagaris-1998.png", dpi=300)
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Alex Zarebski</p>
<p class="date">Created: 2022-07-11 Mon 11:35</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
